{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"198dc3c27f1842a4ac7ba1494b947530":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bba135892fe4824be713e231e84bc01","IPY_MODEL_c3b74342cecf430497d6f9af7baa403b","IPY_MODEL_63b3ec9f6cf044e78ebde1bbc855c780"],"layout":"IPY_MODEL_753496c8620d4768a1b898c2475b8a0d"}},"8bba135892fe4824be713e231e84bc01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a887c5481ea464da4192c564c5ccb0d","placeholder":"​","style":"IPY_MODEL_e5ba21acc5c64297b96c51e4278f06aa","value":"100%"}},"c3b74342cecf430497d6f9af7baa403b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_755a7ca8cd904d0dabbcef6444b0fd68","max":6387309499,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f68b4ea07bc244b89fe75c5c6930ca0b","value":6387309499}},"63b3ec9f6cf044e78ebde1bbc855c780":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6e5ce4c60ee4e0e9dbf6d72aa64dbe2","placeholder":"​","style":"IPY_MODEL_1751fdb28b8b4d46aa51bbbaf8630ecf","value":" 5.95G/5.95G [11:15&lt;00:00, 5.28MB/s]"}},"753496c8620d4768a1b898c2475b8a0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a887c5481ea464da4192c564c5ccb0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5ba21acc5c64297b96c51e4278f06aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"755a7ca8cd904d0dabbcef6444b0fd68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f68b4ea07bc244b89fe75c5c6930ca0b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6e5ce4c60ee4e0e9dbf6d72aa64dbe2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1751fdb28b8b4d46aa51bbbaf8630ecf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6813821a4ee458e8179202c36203a1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d595faeb45b9409caaaef9c2f9a41a19","IPY_MODEL_2469fcaa888849229b72679de2106754","IPY_MODEL_7a2474d1af5c49edb99c69fe900ecf20"],"layout":"IPY_MODEL_68485cbad4ff41a1adeef482c722f2d4"}},"d595faeb45b9409caaaef9c2f9a41a19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a185184534a48c38bdb24bef00a81b3","placeholder":"​","style":"IPY_MODEL_bf8cf26c0ae444b68d019c5877447c6c","value":"100%"}},"2469fcaa888849229b72679de2106754":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2e698c55def4cdc929f927197a4d310","max":346663984,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0679232f228c4286b4d4dcefdffff467","value":346663984}},"7a2474d1af5c49edb99c69fe900ecf20":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5802a62697f446879c7be878671d9515","placeholder":"​","style":"IPY_MODEL_74aae13207b2448e886cc93e8d75ff87","value":" 331M/331M [00:19&lt;00:00, 19.5MB/s]"}},"68485cbad4ff41a1adeef482c722f2d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a185184534a48c38bdb24bef00a81b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf8cf26c0ae444b68d019c5877447c6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2e698c55def4cdc929f927197a4d310":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0679232f228c4286b4d4dcefdffff467":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5802a62697f446879c7be878671d9515":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74aae13207b2448e886cc93e8d75ff87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","metadata":{"id":"hbFBOE8uwJ7V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9e0a0b6-50eb-42fd-e108-4cab128d329d","executionInfo":{"status":"ok","timestamp":1673117925596,"user_tz":-540,"elapsed":6546,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["!pip install torch\n","!pip install torchaudio"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n","Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchaudio) (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchaudio) (4.4.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"3TqGPsmzKiUY"},"source":[]},{"cell_type":"markdown","metadata":{"id":"6iN54BqnSTpk"},"source":["\n","### Reference \n","\n","- Digital Signal Processing Lecture\n","https://github.com/spatialaudio/digital-signal-processing-lecture \n","\n","- Python for Signal Processing (unipingco)\n","https://github.com/unpingco/Python-for-Signal-Processing \n","\n","- Audio for Deep Learning (남기현님)\n","https://tykimos.github.io/2019/07/04/ISS_2nd_Deep_Learning_Conference_All_Together/ \n","\n","- 오디오 전처리 작업을 위한 연습 (박수철님)\n","https://github.com/scpark20/audio-preprocessing-practice \n","\n","- Musical Applications of Machine Learning\n","https://mac.kaist.ac.kr/~juhan/gct634/ \n","\n","- Awesome audio study materials for Korean (최근우님)\n","https://github.com/keunwoochoi/awesome-audio-study-materials-for-korean"]},{"cell_type":"code","metadata":{"id":"F4hfYNb2wu9m","executionInfo":{"status":"ok","timestamp":1673117928478,"user_tz":-540,"elapsed":2885,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bb06mH8jxGtr","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["198dc3c27f1842a4ac7ba1494b947530","8bba135892fe4824be713e231e84bc01","c3b74342cecf430497d6f9af7baa403b","63b3ec9f6cf044e78ebde1bbc855c780","753496c8620d4768a1b898c2475b8a0d","3a887c5481ea464da4192c564c5ccb0d","e5ba21acc5c64297b96c51e4278f06aa","755a7ca8cd904d0dabbcef6444b0fd68","f68b4ea07bc244b89fe75c5c6930ca0b","e6e5ce4c60ee4e0e9dbf6d72aa64dbe2","1751fdb28b8b4d46aa51bbbaf8630ecf","f6813821a4ee458e8179202c36203a1f","d595faeb45b9409caaaef9c2f9a41a19","2469fcaa888849229b72679de2106754","7a2474d1af5c49edb99c69fe900ecf20","68485cbad4ff41a1adeef482c722f2d4","9a185184534a48c38bdb24bef00a81b3","bf8cf26c0ae444b68d019c5877447c6c","c2e698c55def4cdc929f927197a4d310","0679232f228c4286b4d4dcefdffff467","5802a62697f446879c7be878671d9515","74aae13207b2448e886cc93e8d75ff87"]},"outputId":"2c988d88-2fd5-4fb7-dd66-404874b2add8","executionInfo":{"status":"ok","timestamp":1673118658849,"user_tz":-540,"elapsed":729611,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True) \n","test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/5.95G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"198dc3c27f1842a4ac7ba1494b947530"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/331M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6813821a4ee458e8179202c36203a1f"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"N8JvgqogxgtA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4589a11c-9c06-4dfe-e658-41cba91fd2e6","executionInfo":{"status":"ok","timestamp":1673119013585,"user_tz":-540,"elapsed":271,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["test_dataset[1]\n","#소리 데이터, 샘플레이트 "],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.0010,  0.0011,  0.0009,  ..., -0.0007, -0.0010, -0.0011]]),\n"," 16000,\n"," 'STUFF IT INTO YOU HIS BELLY COUNSELLED HIM',\n"," 1089,\n"," 134686,\n"," 1)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"wlBQJ2QopRFk","executionInfo":{"status":"ok","timestamp":1673119020571,"user_tz":-540,"elapsed":283,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["def avg_wer(wer_scores, combined_ref_len):\n","    return float(sum(wer_scores)) / float(combined_ref_len)\n","\n","\n","def _levenshtein_distance(ref, hyp):\n","    \"\"\"\"Levenshtein distance\"는 두 시퀀스 간의 차이를 측정하기위한 문자열 메트릭입니다. \n","    \"Levenshtein distanc\"는 한 단어를 다른 단어로 변경하는 데 필요한 최소 한 문자 편집 (대체, 삽입 또는 삭제) 수로 정의됩니다. \n","    \"\"\"\n","    m = len(ref)\n","    n = len(hyp)\n","\n","    # special case\n","    if ref == hyp:\n","        return 0\n","    if m == 0:\n","        return n\n","    if n == 0:\n","        return m\n","\n","    if m < n:\n","        ref, hyp = hyp, ref\n","        m, n = n, m\n","\n","    # use O(min(m, n)) space\n","    distance = np.zeros((2, n + 1), dtype=np.int32)\n","\n","    # initialize distance matrix\n","    for j in range(0,n + 1):\n","        distance[0][j] = j\n","\n","    # calculate levenshtein distance\n","    for i in range(1, m + 1):\n","        prev_row_idx = (i - 1) % 2\n","        cur_row_idx = i % 2\n","        distance[cur_row_idx][0] = i\n","        for j in range(1, n + 1):\n","            if ref[i - 1] == hyp[j - 1]:\n","                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n","            else:\n","                s_num = distance[prev_row_idx][j - 1] + 1\n","                i_num = distance[cur_row_idx][j - 1] + 1\n","                d_num = distance[prev_row_idx][j] + 1\n","                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n","\n","    return distance[m % 2][n]\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ru55tshPqejt","executionInfo":{"status":"ok","timestamp":1673119026461,"user_tz":-540,"elapsed":1,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["\n","def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"참조 시퀀스와 가설 시퀀스 사이의 거리를 단어 수준으로 계산합니다.\n","     : param reference : 참조 문장.\n","     : param hypothesis : 가설 문장.\n","     : param ignore_case : 대소 문자 구분 여부.\n","     : param delimiter : 입력 문장의 구분자.\n","    \"\"\"\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    ref_words = reference.split(delimiter)\n","    hyp_words = hypothesis.split(delimiter)\n","\n","    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n","    return float(edit_distance), len(ref_words)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-LCRpXVqhv8","executionInfo":{"status":"ok","timestamp":1673119028447,"user_tz":-540,"elapsed":263,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    join_char = ' '\n","    if remove_space == True:\n","        join_char = ''\n","\n","    reference = join_char.join(filter(None, reference.split(' ')))\n","    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n","\n","    edit_distance = _levenshtein_distance(reference, hypothesis)\n","    return float(edit_distance), len(reference)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gBZ_3-Sqm5D","executionInfo":{"status":"ok","timestamp":1673119030526,"user_tz":-540,"elapsed":251,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"Calculate word error rate (WER). \n","    WER = (Sw + Dw + Iw) / Nw\n","    Sw는 대체 된 단어의 수입니다.\n","    Dw는 삭제 된 단어의 수입니다.\n","    Iw는 삽입 된 단어의 수입니다.\n","    Nw는 참조의 단어 수입니다.\n","    \"\"\"\n","    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n","                                         delimiter)\n","    if ref_len == 0:\n","        raise ValueError(\"Reference's word number should be greater than 0.\")\n","\n","    wer = float(edit_distance) / ref_len\n","    return wer"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6YUEk-2qqaw","executionInfo":{"status":"ok","timestamp":1673119032947,"user_tz":-540,"elapsed":252,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n","    \"\"\"Calculate charactor error rate (CER). \n","        CER = (Sc + Dc + Ic) / Nc\n","        Sc is the number of characters substituted,\n","        Dc is the number of characters deleted,\n","        Ic is the number of characters inserted\n","        Nc is the number of characters in the reference\n","    \"\"\"\n","    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n","                                         remove_space)\n","\n","    if ref_len == 0:\n","        raise ValueError(\"Length of reference should be greater than 0.\")\n","\n","    cer = float(edit_distance) / ref_len\n","    return cer"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"cS2jhIrBqs_G","executionInfo":{"status":"ok","timestamp":1673119035603,"user_tz":-540,"elapsed":262,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["class TextTransform:\n","    \"\"\"Maps characters to integers and vice versa\"\"\"\n","    def __init__(self):\n","        char_map_str = \"\"\"\n","        ' 0\n","        <SPACE> 1\n","        a 2\n","        b 3\n","        c 4\n","        d 5\n","        e 6\n","        f 7\n","        g 8\n","        h 9\n","        i 10\n","        j 11\n","        k 12\n","        l 13\n","        m 14\n","        n 15\n","        o 16\n","        p 17\n","        q 18\n","        r 19\n","        s 20\n","        t 21\n","        u 22\n","        v 23\n","        w 24\n","        x 25\n","        y 26\n","        z 27\n","        \"\"\"\n","        self.char_map = {}\n","        self.index_map = {}\n","        for line in char_map_str.strip().split('\\n'):\n","            ch, index = line.split()\n","            self.char_map[ch] = int(index)\n","            self.index_map[int(index)] = ch\n","        self.index_map[1] = ' '\n","\n","    def text_to_int(self, text):\n","        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n","        int_sequence = []\n","        for c in text:\n","            if c == ' ':\n","                ch = self.char_map['<SPACE>']\n","            else:\n","                ch = self.char_map[c]\n","            int_sequence.append(ch)\n","        return int_sequence\n","\n","    def int_to_text(self, labels):\n","        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n","        string = []\n","        for i in labels:\n","            string.append(self.index_map[i])\n","        return ''.join(string).replace('<SPACE>', ' ')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMUWPlL8qwQg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d2dfe251-8fa0-4438-cf34-677bde135c13","executionInfo":{"status":"ok","timestamp":1673119040539,"user_tz":-540,"elapsed":235,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["train_audio_transforms = nn.Sequential(\n","    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n","    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n","    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",")\n","\n","valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n","\n","text_transform = TextTransform()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchaudio/functional/functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n","  warnings.warn(\n"]}]},{"cell_type":"code","metadata":{"id":"VrbecqsFq8qM","executionInfo":{"status":"ok","timestamp":1673119046036,"user_tz":-540,"elapsed":249,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["train_audio_transforms = nn.Sequential(\n","    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n","    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n","    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",")\n","\n","valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n","\n","text_transform = TextTransform()\n","\n","def data_processing(data, data_type=\"train\"):\n","    spectrograms = []\n","    labels = []\n","    input_lengths = []\n","    label_lengths = []\n","    for (waveform, _, utterance, _, _, _) in data:\n","        if data_type == 'train':\n","            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        elif data_type == 'valid':\n","            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        else:\n","            raise Exception('data_type should be train or valid')\n","        spectrograms.append(spec)\n","        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n","        labels.append(label)\n","        input_lengths.append(spec.shape[0]//2)\n","        label_lengths.append(len(label))\n","\n","    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n","    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n","\n","    return spectrograms, labels, input_lengths, label_lengths\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXAYnQ4arD2P","executionInfo":{"status":"ok","timestamp":1673119051766,"user_tz":-540,"elapsed":238,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_layers=1,\n","                 input_dropout_p=0, dropout_p=0,\n","                 bidirectional=False, rnn_cell='gru', variable_lengths=False):\n","\n","        self.hidden_size = hidden_size\n","        self.bidirectional = bidirectional\n","        self.n_layers = n_layers\n","        self.dropout_p = dropout_p\n","        self.variable_lengths = variable_lengths\n","\n","        if rnn_cell.lower() == 'lstm':\n","            self.rnn_cell = nn.LSTM\n","        elif rnn_cell.lower() == 'gru':\n","            self.rnn_cell = nn.GRU\n","        else:\n","            raise ValueError(\"Unsupported RNN Cell: {0}\".format(rnn_cell))\n","\n","        \"\"\"\n","        Copied from https://github.com/SeanNaren/deepspeech.pytorch/blob/master/model.py\n","        Copyright (c) 2017 Sean Naren\n","        MIT License\n","        \"\"\"\n","        outputs_channel = 32\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, outputs_channel, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n","            nn.BatchNorm2d(outputs_channel),\n","            nn.Hardtanh(0, 20, inplace=True),\n","            nn.Conv2d(outputs_channel, outputs_channel, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n","            nn.BatchNorm2d(outputs_channel),\n","            nn.Hardtanh(0, 20, inplace=True)\n","        )\n","\n","        rnn_input_dims = int(math.floor(input_size + 2 * 20 - 41) / 2 + 1)\n","        rnn_input_dims = int(math.floor(rnn_input_dims + 2 * 10 - 21) / 2 + 1)\n","        rnn_input_dims *= outputs_channel\n","        \n","        self.rnn =  self.rnn_cell(rnn_input_dims, self.hidden_size, self.n_layers, dropout=self.dropout_p, bidirectional=self.bidirectional)\n","\n","\n","    def forward(self, input_var, input_lengths=None):\n","        \"\"\"\n","        param:input_var: Encoder inputs, Spectrogram, Shape=(B,1,D,T)\n","        param:input_lengths: inputs sequence length without zero-pad\n","        \"\"\"\n","        output_lengths = self.get_seq_lens(input_lengths)\n","\n","        x = input_var # (B,1,D,T)\n","        x, _ = self.conv(x, output_lengths) # (B, C, D, T)\n","        \n","        x_size = x.size()\n","        x = x.view(x_size[0], x_size[1] * x_size[2], x_size[3]) # (B, C * D, T)\n","        x = x.transpose(1, 2).transpose(0, 1).contiguous() # (T, B, D)\n","\n","        x = nn.utils.rnn.pack_padded_sequence(x, output_lengths)\n","        x, h_state = self.rnn(x)\n","        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n","        x = x.transpose(0, 1) # (B, T, D)\n","        return x, h_state\n","\n","\n","    def get_seq_lens(self, input_length):\n","        seq_len = input_length\n","        for m in self.conv.modules():\n","            if type(m) == nn.modules.conv.Conv2d :\n","                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n","\n","        return seq_len.int()"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMITaH3ALmr8","executionInfo":{"status":"ok","timestamp":1673119056806,"user_tz":-540,"elapsed":243,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Attention(nn.Module):\n","    def __init__(self, dec_dim, enc_dim, conv_dim, attn_dim):\n","        super(Attention, self).__init__()\n","        self.dec_dim = dec_dim\n","        self.enc_dim = enc_dim\n","        self.conv_dim = conv_dim\n","        self.attn_dim = attn_dim\n","        self.conv = nn.Conv1d(in_channels=1, out_channels=self.attn_dim, kernel_size=3, padding=1)\n","\n","        self.W = nn.Linear(self.dec_dim, self.attn_dim, bias=False)\n","        self.V = nn.Linear(self.enc_dim, self.attn_dim, bias=False)\n","\n","        self.fc = nn.Linear(attn_dim, 1, bias=True)\n","        self.b = nn.Parameter(torch.rand(attn_dim))\n","\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, queries, values, last_attn):\n","        \"\"\"\n","        param:quries: Decoder hidden states, Shape=(B,1,dec_D)\n","        param:values: Encoder outputs, Shape=(B,enc_T,enc_D)\n","        param:last_attn: Attention weight of previous step, Shape=(batch, enc_T)\n","        \"\"\"\n","        batch_size = queries.size(0)\n","        dec_feat_dim = queries.size(2)\n","        enc_feat_len = values.size(1)\n","\n","        # conv_attn = (B, enc_T, conv_D)\n","        conv_attn = torch.transpose(self.conv(last_attn.unsqueeze(dim=1)), 1, 2)\n","\n","        # (B, enc_T)\n","        score =  self.fc(self.tanh(\n","         self.W(queries) + self.V(values) + conv_attn + self.b\n","        )).squeeze(dim=-1)\n","\n","        attn_weight = self.softmax(score) \n","        # (B, 1, enc_T) * (B, enc_T, enc_D) -> (B, 1, enc_D) \n","        context = torch.bmm(attn_weight.unsqueeze(dim=1), values)\n","        return context, attn_weight"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0wPUMERrnjX","executionInfo":{"status":"ok","timestamp":1673119064486,"user_tz":-540,"elapsed":251,"user":{"displayName":"김정인","userId":"08669711361714769540"}}},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, vocab_size, max_len, hidden_size, encoder_size,\n","                 sos_id, eos_id,\n","                 n_layers=1, rnn_cell='gru', \n","                 bidirectional_encoder=False, bidirectional_decoder=False,\n","                 dropout_p=0, use_attention=True):\n","\n","        self.output_size = vocab_size\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.bidirectional_encoder = bidirectional_encoder\n","        self.bidirectional_decoder = bidirectional_decoder\n","        self.encoder_output_size = encoder_size * 2 if self.bidirectional_encoder else encoder_size\n","        self.n_layers = n_layers\n","        self.dropout_p = dropout_p\n","        self.max_length = max_len\n","        self.use_attention = use_attention\n","        self.eos_id = eos_id\n","        self.sos_id = sos_id\n","        \n","        if rnn_cell.lower() == 'lstm':\n","            self.rnn_cell = nn.LSTM\n","        elif rnn_cell.lower() == 'gru':\n","            self.rnn_cell = nn.GRU\n","        else:\n","            raise ValueError(\"Unsupported RNN Cell: {0}\".format(rnn_cell))\n","\n","        self.init_input = None\n","        self.rnn = self.rnn_cell(self.hidden_size + self.encoder_output_size, self.hidden_size, self.n_layers,\n","                                 batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional_decoder)\n","\n","        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.attention = Attention(dec_dim=self.hidden_size, enc_dim=self.encoder_output_size, conv_dim=1, attn_dim=self.hidden_size)\n","        self.fc = nn.Linear(self.hidden_size + self.encoder_output_size, self.output_size)\n","\n","    def forward_step(self, input_var, hidden, encoder_outputs, context, attn_w, function):\n","        if self.training:\n","            self.rnn.flatten_parameters()\n","\n","        batch_size = input_var.size(0)\n","        dec_len = input_var.size(1)\n","        enc_len = encoder_outputs.size(1)\n","        enc_dim = encoder_outputs.size(2)\n","        embedded = self.embedding(input_var) # (B, dec_T, voc_D) -> (B, dec_T, dec_D)\n","        embedded = self.input_dropout(embedded)\n","\n","        y_all = []\n","        attn_w_all = []\n","        for i in range(embedded.size(1)):\n","            embedded_inputs = embedded[:, i, :] # (B, dec_D)\n","            \n","            rnn_input = torch.cat([embedded_inputs, context], dim=1) # (B, dec_D + enc_D)\n","            rnn_input = rnn_input.unsqueeze(1) \n","            output, hidden = self.rnn(rnn_input, hidden) # (B, 1, dec_D)\n","\n","            context, attn_w = self.attention(output, encoder_outputs, attn_w) # (B, 1, enc_D), (B, enc_T)\n","            attn_w_all.append(attn_w)\n","            \n","            context = context.squeeze(1)\n","            output = output.squeeze(1) # (B, 1, dec_D) -> (B, dec_D)\n","            context = self.input_dropout(context)\n","            output = self.input_dropout(output)\n","            output = torch.cat((output, context), dim=1) # (B, dec_D + enc_D)\n","\n","            pred = function(self.fc(output), dim=-1)\n","            y_all.append(pred)\n","\n","        return y_all, hidden, context, attn_w_all\n","\n","\n","    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None,\n","                    function=F.log_softmax):\n","        \"\"\"\n","        param:inputs: Decoder inputs sequence, Shape=(B, dec_T)\n","        param:encoder_hidden: Encoder last hidden states, Default : None\n","        param:encoder_outputs: Encoder outputs, Shape=(B,enc_T,enc_D)\n","        \"\"\"\n","    \n","        batch_size = encoder_outputs.size(0)\n","        inputs = torch.LongTensor([self.sos_id] * batch_size).view(batch_size, 1)\n","        max_length = self.max_length\n","\n","        decoder_hidden = None\n","        context = encoder_outputs.new_zeros(batch_size, encoder_outputs.size(2)) # (B, D)\n","        attn_w = encoder_outputs.new_zeros(batch_size, encoder_outputs.size(1)) # (B, T)\n","\n","        decoder_outputs = []\n","        sequence_symbols = []\n","        lengths = np.array([max_length] * batch_size)\n","\n","        def decode(step, step_output):\n","            decoder_outputs.append(step_output)\n","            symbols = decoder_outputs[-1].topk(1)[1]\n","            sequence_symbols.append(symbols)\n","            eos_batches = symbols.data.eq(self.eos_id)\n","            if eos_batches.dim() > 0:\n","                eos_batches = eos_batches.cpu().view(-1).numpy()\n","                update_idx = ((lengths > step) & eos_batches) != 0\n","                lengths[update_idx] = len(sequence_symbols)\n","            return symbols\n","\n","        decoder_input = inputs[:, 0].unsqueeze(1)\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden, context, attn_w = self.forward_step(decoder_input, \n","                                                                                decoder_hidden,\n","                                                                                encoder_outputs,\n","                                                                                context,\n","                                                                                attn_w,\n","                                                                                function=function)\n","            step_output = decoder_output.squeeze(1)\n","            symbols = decode(di, step_output)\n","            decoder_input = symbols\n","\n","        return decoder_outputs"],"execution_count":15,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hJRZ8jn2YdcJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyojINGV98zz"},"source":[],"execution_count":null,"outputs":[]}]}