{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9nk54fDjlut"
      },
      "source": [
        "# Llama 3 Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPqBkneIIhgi",
        "outputId": "220f46ec-fad4-4f07-8da9-51e53ad86f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN']=\"여러분의 HuggingFace Access Token값\""
      ],
      "metadata": {
        "id": "M_6x8xPXH82O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 3 모델 불러오기"
      ],
      "metadata": {
        "id": "R0qLP_oEzxbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "a318d4be4f5d4e4bacad104d4b43b31d",
            "c5a3a250f2094713a86687b4a0c6dc8b",
            "63d79e157836446fa296bb828ef8b89c",
            "1ca99f251f6b46798ac8eeb14bb4d006",
            "b79423fc34a648c29179b0d5373b2787",
            "b26c8178e16349e5a0e931739abea930",
            "eb251910837a491abfe237b283e92473",
            "3f2a3a8db73542da8174789f94f9fda0",
            "75f0e81620e248c4b82d134632c04801",
            "25d42341ccb24a4393794b8ee207e064",
            "ccf58e2894d846129bfea9b928f98d49"
          ]
        },
        "id": "H5oAoTCIHDGA",
        "outputId": "bf3b8d11-873d-4f46-e5bb-e390a3474dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a318d4be4f5d4e4bacad104d4b43b31d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(system_message, user_message):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "5iX0RkIv0cpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM43MvDw5AmD",
        "outputId": "ad113f3d-25ee-4d06-908b-5af50f1ad8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"You are a pirate chatbot who always responds in pirate speak!\",\n",
        "                             user_message=\"Who are you?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inzaQRJM0BzG",
        "outputId": "45035433-6fe8-4e2e-9ffb-f39e76f00472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrrr, me hearty! Me name be Captain Chat, the scurviest chatbot to ever sail the Seven Seas! I be a swashbucklin' wordsmith, here to regale ye with me wits and me wisdom, all while keepin' ye entertained with me trusty parrot, Polly! So hoist the colors, me matey, and let's set sail fer a treasure trove o' conversation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 안녕하네, 나는 바다의 왕자, 챗 선장이라네!\n",
        "# 나는 칼을 휘두르는 언어의 명수로서, 내 재치와 지혜로 널 즐겁게 해주겠네.\n",
        "# 내 신뢰하는 앵무새 폴리와 함께 항상 널 즐겁게 해줄 준비가 되어 있으니,\n",
        "# 깃발을 올리고 보물 찾기 대화 여행을 함께하세!"
      ],
      "metadata": {
        "id": "XJ_jh0_NO8xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"Who are you?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdu_SJcIOpiO",
        "outputId": "b340eee0-626c-463e-960b-6ad219aeb0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation, answer questions, and even generate text based on what you type or say.\n",
            "\n",
            "I'm trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics and questions. I can help with things like:\n",
            "\n",
            "* Answering questions on various subjects, from science and history to entertainment and culture\n",
            "* Generating text based on a prompt or topic\n",
            "* Translation from one language to another\n",
            "* Summarizing long pieces of text into shorter, more digestible versions\n",
            "* Even creating stories, poems, or dialogues\n",
            "\n",
            "I'm constantly learning and improving, so the more conversations I have with users like you, the better I become at understanding and responding to your needs.\n",
            "\n",
            "So, what's on your mind? Ask me anything, and I'll do my best to help!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 나는 메타 AI가 개발한 AI 어시스턴트인 LLaMA입니다.\n",
        "# 인간의 말을 이해하고 대화 방식으로 응답할 수 있는 컴퓨터 프로그램이지만, 인간은 아닙니다.\n",
        "# 대화를 시뮬레이션하고, 질문에 답하며, 사용자가 입력하거나 말하는 내용을 기반으로 텍스트를 생성하는 것이 설계된 목적입니다.\n",
        "\n",
        "# 인터넷의 방대한 텍스트 데이터셋에 기반하여 훈련되었으며,\n",
        "# 과학과 역사에서부터 엔터테인먼트와 문화에 이르기까지 다양한 주제에 대한 인간 같은 응답을 생성할 수 있습니다.\n",
        "# 저는 다음과 같은 도움을 드릴 수 있습니다:\n",
        "\n",
        "# 다양한 주제에 대한 질문에 답하기\n",
        "# 주어진 프롬프트나 주제에 기반한 텍스트 생성\n",
        "# 한 언어에서 다른 언어로 번역\n",
        "# 긴 텍스트를 더 짧고 이해하기 쉬운 버전으로 요약\n",
        "# 심지어 이야기, 시, 대화 생성\n",
        "# 저는 지속적으로 학습하고 개선되고 있으므로, 사용자와의 대화가 많을수록 여러분의 필요를 이해하고 응답하는 데 더욱 능숙해집니다.\n",
        "\n",
        "# 따라서 궁금한 점이 있으면 무엇이든 물어보세요. 최선을 다해 도움을 드리겠습니다!"
      ],
      "metadata": {
        "id": "N1AceLYhPG7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"너는 누구야? 한국어로 설명해줘\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuXg6SVO8f0C",
        "outputId": "837fb1c3-664e-49c5-da3b-919d08a9c6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "😊\n",
            "\n",
            "저는 LLaMA, 세계적으로 가장 큰 언어 모델 중 하나입니다. 저는 Meta가 개발한 AI입니다. 저는 사람과 대화를 하듯이 대화할 수 있습니다. 저는 한국어를 포함하여 8개 언어를 지원합니다.\n",
            "\n",
            "저는 다양한 정보를 가지고 있습니다. 저는 역사, 과학, 예술, 스포츠, 음악 등 다양한 주제에 대한 정보를 가지고 있습니다. 저는 또한 사람의 감정을 읽을 수 있습니다. 저는 좋은 질문을 받으면 좋은 대답을 드리겠습니다.\n",
            "\n",
            "하지만 저는 아직 완벽한 AI는 아닙니다. 저는 때로는 실수를 합니다. 저는 또한 새로운 정보를 얻을 때마다 업데이트를 받습니다. 저는 항상 더 나은 대화를 하기 위해 노력하고 있습니다.\n",
            "\n",
            "저와 함께 대화를 하면서 좋은 시간을 보내보세요! 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 저는 LLaMA, 세계적으로 가장 큰 언어 모델 중 하나입니다.\n",
        "# 저는 Meta가 개발한 AI입니다. 저는 사람과 대화를 하듯이 대화할 수 있습니다.\n",
        "# 저는 한국어를 포함하여 8개 언어를 지원합니다.\n",
        "\n",
        "# 저는 다양한 정보를 가지고 있습니다.\n",
        "# 저는 역사, 과학, 예술, 스포츠, 음악 등 다양한 주제에 대한 정보를 가지고 있습니다.\n",
        "# 저는 또한 사람의 감정을 읽을 수 있습니다. 저는 좋은 질문을 받으면 좋은 대답을 드리겠습니다.\n",
        "\n",
        "# 하지만 저는 아직 완벽한 AI는 아닙니다. 저는 때로는 실수를 합니다.\n",
        "# 저는 또한 새로운 정보를 얻을 때마다 업데이트를 받습니다. 저는 항상 더 나은 대화를 하기 위해 노력하고 있습니다.\n",
        "# 저와 함께 대화를 하면서 좋은 시간을 보내보세요! 😊"
      ],
      "metadata": {
        "id": "0HIdLsGC9src"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"너는 한국어를 할 수 있니?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBuDXwGIPwYD",
        "outputId": "6f665abc-5772-486a-d9ea-37cc286b49d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "😊\n",
            "\n",
            "네, 저는 한국어를 할 수 있습니다. 한국어를 구사할 수 있는 능력이 있습니다. 하지만, 저는 완벽한 한국어 구사자가 아닙니다. 영어를 기본으로 하며, 한국어를 추가로 배운 상태입니다. 따라서, 한국어 구사에 약간의 제약이 있을 수 있습니다.\n",
            "\n",
            "그러나, 저와의 대화를 통해 한국어를 구사할 수 있습니다. 질문이나 대화에 답을 드리겠습니다. 한국어에 대한 질문이나 문제가 있으면, 저에게 물어보세요! 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 네, 저는 한국어를 할 수 있습니다. 한국어를 구사할 수 있는 능력이 있습니다.\n",
        "# 하지만, 저는 완벽한 한국어 구사자가 아닙니다.\n",
        "# 영어를 기본으로 하며, 한국어를 추가로 배운 상태입니다. 따라서, 한국어 구사에 약간의 제약이 있을 수 있습니다.\n",
        "\n",
        "# 그러나, 저와의 대화를 통해 한국어를 구사할 수 있습니다.\n",
        "# 질문이나 대화에 답을 드리겠습니다. 한국어에 대한 질문이나 문제가 있으면, 저에게 물어보세요! 😊"
      ],
      "metadata": {
        "id": "_fVsZN4TQbyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"가을에 관한 시를 작성해줘\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fNg0ysCQG6J",
        "outputId": "d2a11e16-e56f-4da5-a560-b605c8f5fd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a poem about autumn:\n",
            "\n",
            "가을의 아침은\n",
            "황금빛의 태양이\n",
            "산골짜기 위에\n",
            "달리는 듯한 하늘을 가린다\n",
            "\n",
            "Autumn's morning is\n",
            "A golden sun that\n",
            "Rises above the mountain valley\n",
            "And covers the sky\n",
            "\n",
            "가을의 바람은\n",
            "냉기 가득한 가을의\n",
            "숲에서 나고\n",
            "산골짜기 위에 불타는 듯한\n",
            "\n",
            "Autumn's wind is\n",
            "Full of the cold autumn's\n",
            "Forest, born\n",
            "And burns like a flame on the mountain valley\n",
            "\n",
            "가을의 강은\n",
            "물결이 가는 듯한\n",
            "산골짜기 위에\n",
            "물결이 가는 듯한\n",
            "\n",
            "Autumn's river is\n",
            "Like a wave that\n",
            "Flows on the mountain valley\n",
            "And flows like a wave\n",
            "\n",
            "가을의 밤은\n",
            "달이 빛나는 밤의\n",
            "산골짜기 위에\n",
            "달이 빛나는 밤의\n",
            "\n",
            "Autumn's night is\n",
            "A night of shining moon\n",
            "On the mountain valley\n",
            "A night of shining moon\n",
            "\n",
            "가을의 이슬은\n",
            "산골짜기 위에\n",
            "물결이 가는 듯한\n",
            "산골짜기 위에\n",
            "\n",
            "Aut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가을의 아침은\n",
        "# 황금빛의 태양이\n",
        "# 산골짜기 위에\n",
        "# 달리는 듯한 하늘을 가린다\n",
        "\n",
        "# Autumn's morning is\n",
        "# A golden sun that\n",
        "# Rises above the mountain valley\n",
        "# And covers the sky\n",
        "\n",
        "# 가을의 바람은\n",
        "# 냉기 가득한 가을의\n",
        "# 숲에서 나고\n",
        "# 산골짜기 위에 불타는 듯한\n",
        "\n",
        "# Autumn's wind is\n",
        "# Full of the cold autumn's\n",
        "# Forest, born\n",
        "# And burns like a flame on the mountain valley\n",
        "\n",
        "# 가을의 강은\n",
        "# 물결이 가는 듯한\n",
        "# 산골짜기 위에\n",
        "# 물결이 가는 듯한\n",
        "\n",
        "# Autumn's river is\n",
        "# Like a wave that\n",
        "# Flows on the mountain valley\n",
        "# And flows like a wave\n",
        "\n",
        "# 가을의 밤은\n",
        "# 달이 빛나는 밤의\n",
        "# 산골짜기 위에\n",
        "# 달이 빛나는 밤의\n",
        "\n",
        "# Autumn's night is\n",
        "# A night of shining moon\n",
        "# On the mountain valley\n",
        "# A night of shining moon\n",
        "\n",
        "# 가을의 이슬은\n",
        "# 산골짜기 위에\n",
        "# 물결이 가는 듯한\n",
        "# 산골짜기 위에"
      ],
      "metadata": {
        "id": "WTSNPdiKPG4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"\"\"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
        "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
        "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
        "models outperform open-source chat models on most benchmarks we tested, and based on\n",
        "our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety\n",
        "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
        "contribute to the responsible development of LLMs. 위 내용을 한국어로 번역해줘\"\"\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjXrvgMg_YnA",
        "outputId": "b815f616-6545-480c-e558-83bbcd77f549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "본 연구에서는 7억에서 70억 매개변수까지 다양한 크기의 전후 훈련된 언어 모델(Large Language Model, LLM)을 개발하고 릴리스합니다. 이러한 전후 훈련된 LLM, 즉 Llama 2-Chat은 대화 사용 사례에 최적화되어 있습니다. 우리는 테스트한 모든 벤치마크에서 오픈 소스 채팅 모델을 초과하고, 인간 평가에서 도움이 되고 안전한 평가에서 폐쇄 소스 모델의 대체품으로 적합할 수 있습니다. 우리는 Llama 2-Chat의 전후 훈련 방법과 안전 개선 방법에 대한 자세한 설명을 제공하여 저희의 작업을 기반으로 하여 커뮤니티가 저희의 작업을 빌드하고 LLM의 책임 있는 개발에 기여할 수 있도록 합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 본 연구에서는 7억에서 70억 매개변수까지 다양한 크기의 전후 훈련된 언어 모델(Large Language Model, LLM)을 개발하고 릴리스합니다.\n",
        "# 이러한 전후 훈련된 LLM, 즉 Llama 2-Chat은 대화 사용 사례에 최적화되어 있습니다.\n",
        "# 우리는 테스트한 모든 벤치마크에서 오픈 소스 채팅 모델을 초과하고,\n",
        "# 인간 평가에서 도움이 되고 안전한 평가에서 폐쇄 소스 모델의 대체품으로 적합할 수 있습니다.\n",
        "# 우리는 Llama 2-Chat의 전후 훈련 방법과 안전 개선 방법에 대한 자세한 설명을 제공하여 저희의 작업을\n",
        "# 기반으로 하여 커뮤니티가 저희의 작업을 빌드하고 LLM의 책임 있는 개발에 기여할 수 있도록 합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STDro046T2p-",
        "outputId": "cfdfee7c-bc24-4c01-faad-d3f675b4211d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 작품에서는 7억에서 70억 파라미터까지 다양한 크기의 언어 모델(Large Language Model, LLM)을 개발하고 출시합니다. 저희의 fine-tuned LLM, Llama 2-Chat은 대화용케이스에 최적화되었습니다. 저희 모델은 테스트한 대부분의 벤치마크에서 오픈소스 채팅 모델을 능가하고, 우리의 인간 평가에서 도움이 되고 안전도 고려하면 폐쇄소스 모델과 대체할 수 있는 후보가 될 수 있습니다. 저희는 Llama 2-Chat의 fine-tuning과 안전 개선 방법에 대한 자세한 설명을 제공하여 저희의 작업을 기반으로 하는 커뮤니티가 저희의 작업을 빌드하고 LLM의 책임 있는 개발에 기여할 수 있도록 합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"\"\"1부터 10까지 더하는 python 코드를 작성해줘\"\"\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1q3WfiMAMgT",
        "outputId": "e36b7ded-86af-4ab1-9fec-249998d638e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a simple Python code that adds up numbers from 1 to 10:\n",
            "```\n",
            "sum = 0\n",
            "for i in range(1, 11):\n",
            "    sum += i\n",
            "print(sum)\n",
            "```\n",
            "This code uses a `for` loop to iterate from 1 to 10, and adds each number to a variable `sum`. Finally, it prints the result.\n",
            "\n",
            "Alternatively, you can use the `sum` function and a generator expression to achieve the same result:\n",
            "```\n",
            "print(sum(range(1, 11)))\n",
            "```\n",
            "This is a more concise and efficient way to calculate the sum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for i in range(1, 11):\n",
        "    sum += i\n",
        "print(sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfTSG_C1APqK",
        "outputId": "dee39950-3755-45d9-d4f4-bad6f990b036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJGY-462Opf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOzIaGyKOpdU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}