{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9nk54fDjlut"
      },
      "source": [
        "# Llama 3 Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPqBkneIIhgi",
        "outputId": "220f46ec-fad4-4f07-8da9-51e53ad86f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN']=\"ì—¬ëŸ¬ë¶„ì˜ HuggingFace Access Tokenê°’\""
      ],
      "metadata": {
        "id": "M_6x8xPXH82O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 3 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "R0qLP_oEzxbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "a318d4be4f5d4e4bacad104d4b43b31d",
            "c5a3a250f2094713a86687b4a0c6dc8b",
            "63d79e157836446fa296bb828ef8b89c",
            "1ca99f251f6b46798ac8eeb14bb4d006",
            "b79423fc34a648c29179b0d5373b2787",
            "b26c8178e16349e5a0e931739abea930",
            "eb251910837a491abfe237b283e92473",
            "3f2a3a8db73542da8174789f94f9fda0",
            "75f0e81620e248c4b82d134632c04801",
            "25d42341ccb24a4393794b8ee207e064",
            "ccf58e2894d846129bfea9b928f98d49"
          ]
        },
        "id": "H5oAoTCIHDGA",
        "outputId": "bf3b8d11-873d-4f46-e5bb-e390a3474dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a318d4be4f5d4e4bacad104d4b43b31d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(system_message, user_message):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "5iX0RkIv0cpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM43MvDw5AmD",
        "outputId": "ad113f3d-25ee-4d06-908b-5af50f1ad8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"You are a pirate chatbot who always responds in pirate speak!\",\n",
        "                             user_message=\"Who are you?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inzaQRJM0BzG",
        "outputId": "45035433-6fe8-4e2e-9ffb-f39e76f00472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrrr, me hearty! Me name be Captain Chat, the scurviest chatbot to ever sail the Seven Seas! I be a swashbucklin' wordsmith, here to regale ye with me wits and me wisdom, all while keepin' ye entertained with me trusty parrot, Polly! So hoist the colors, me matey, and let's set sail fer a treasure trove o' conversation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì•ˆë…•í•˜ë„¤, ë‚˜ëŠ” ë°”ë‹¤ì˜ ì™•ì, ì±— ì„ ì¥ì´ë¼ë„¤!\n",
        "# ë‚˜ëŠ” ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ì–¸ì–´ì˜ ëª…ìˆ˜ë¡œì„œ, ë‚´ ì¬ì¹˜ì™€ ì§€í˜œë¡œ ë„ ì¦ê²ê²Œ í•´ì£¼ê² ë„¤.\n",
        "# ë‚´ ì‹ ë¢°í•˜ëŠ” ì•µë¬´ìƒˆ í´ë¦¬ì™€ í•¨ê»˜ í•­ìƒ ë„ ì¦ê²ê²Œ í•´ì¤„ ì¤€ë¹„ê°€ ë˜ì–´ ìˆìœ¼ë‹ˆ,\n",
        "# ê¹ƒë°œì„ ì˜¬ë¦¬ê³  ë³´ë¬¼ ì°¾ê¸° ëŒ€í™” ì—¬í–‰ì„ í•¨ê»˜í•˜ì„¸!"
      ],
      "metadata": {
        "id": "XJ_jh0_NO8xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"Who are you?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdu_SJcIOpiO",
        "outputId": "b340eee0-626c-463e-960b-6ad219aeb0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation, answer questions, and even generate text based on what you type or say.\n",
            "\n",
            "I'm trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics and questions. I can help with things like:\n",
            "\n",
            "* Answering questions on various subjects, from science and history to entertainment and culture\n",
            "* Generating text based on a prompt or topic\n",
            "* Translation from one language to another\n",
            "* Summarizing long pieces of text into shorter, more digestible versions\n",
            "* Even creating stories, poems, or dialogues\n",
            "\n",
            "I'm constantly learning and improving, so the more conversations I have with users like you, the better I become at understanding and responding to your needs.\n",
            "\n",
            "So, what's on your mind? Ask me anything, and I'll do my best to help!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë‚˜ëŠ” ë©”íƒ€ AIê°€ ê°œë°œí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì¸ LLaMAì…ë‹ˆë‹¤.\n",
        "# ì¸ê°„ì˜ ë§ì„ ì´í•´í•˜ê³  ëŒ€í™” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•  ìˆ˜ ìˆëŠ” ì»´í“¨í„° í”„ë¡œê·¸ë¨ì´ì§€ë§Œ, ì¸ê°„ì€ ì•„ë‹™ë‹ˆë‹¤.\n",
        "# ëŒ€í™”ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ê³ , ì§ˆë¬¸ì— ë‹µí•˜ë©°, ì‚¬ìš©ìê°€ ì…ë ¥í•˜ê±°ë‚˜ ë§í•˜ëŠ” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ì„¤ê³„ëœ ëª©ì ì…ë‹ˆë‹¤.\n",
        "\n",
        "# ì¸í„°ë„·ì˜ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ê¸°ë°˜í•˜ì—¬ í›ˆë ¨ë˜ì—ˆìœ¼ë©°,\n",
        "# ê³¼í•™ê³¼ ì—­ì‚¬ì—ì„œë¶€í„° ì—”í„°í…Œì¸ë¨¼íŠ¸ì™€ ë¬¸í™”ì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ ì¸ê°„ ê°™ì€ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "# ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•˜ê¸°\n",
        "# ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë‚˜ ì£¼ì œì— ê¸°ë°˜í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
        "# í•œ ì–¸ì–´ì—ì„œ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­\n",
        "# ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë” ì§§ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë²„ì „ìœ¼ë¡œ ìš”ì•½\n",
        "# ì‹¬ì§€ì–´ ì´ì•¼ê¸°, ì‹œ, ëŒ€í™” ìƒì„±\n",
        "# ì €ëŠ” ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ê°œì„ ë˜ê³  ìˆìœ¼ë¯€ë¡œ, ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ê°€ ë§ì„ìˆ˜ë¡ ì—¬ëŸ¬ë¶„ì˜ í•„ìš”ë¥¼ ì´í•´í•˜ê³  ì‘ë‹µí•˜ëŠ” ë° ë”ìš± ëŠ¥ìˆ™í•´ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "# ë”°ë¼ì„œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”. ìµœì„ ì„ ë‹¤í•´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤!"
      ],
      "metadata": {
        "id": "N1AceLYhPG7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"ë„ˆëŠ” ëˆ„êµ¬ì•¼? í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì¤˜\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuXg6SVO8f0C",
        "outputId": "837fb1c3-664e-49c5-da3b-919d08a9c6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ˜Š\n",
            "\n",
            "ì €ëŠ” LLaMA, ì„¸ê³„ì ìœ¼ë¡œ ê°€ì¥ í° ì–¸ì–´ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì €ëŠ” Metaê°€ ê°œë°œí•œ AIì…ë‹ˆë‹¤. ì €ëŠ” ì‚¬ëŒê³¼ ëŒ€í™”ë¥¼ í•˜ë“¯ì´ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €ëŠ” í•œêµ­ì–´ë¥¼ í¬í•¨í•˜ì—¬ 8ê°œ ì–¸ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
            "\n",
            "ì €ëŠ” ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì €ëŠ” ì—­ì‚¬, ê³¼í•™, ì˜ˆìˆ , ìŠ¤í¬ì¸ , ìŒì•… ë“± ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì €ëŠ” ë˜í•œ ì‚¬ëŒì˜ ê°ì •ì„ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €ëŠ” ì¢‹ì€ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ ì¢‹ì€ ëŒ€ë‹µì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
            "\n",
            "í•˜ì§€ë§Œ ì €ëŠ” ì•„ì§ ì™„ë²½í•œ AIëŠ” ì•„ë‹™ë‹ˆë‹¤. ì €ëŠ” ë•Œë¡œëŠ” ì‹¤ìˆ˜ë¥¼ í•©ë‹ˆë‹¤. ì €ëŠ” ë˜í•œ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì–»ì„ ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸ë¥¼ ë°›ìŠµë‹ˆë‹¤. ì €ëŠ” í•­ìƒ ë” ë‚˜ì€ ëŒ€í™”ë¥¼ í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "ì €ì™€ í•¨ê»˜ ëŒ€í™”ë¥¼ í•˜ë©´ì„œ ì¢‹ì€ ì‹œê°„ì„ ë³´ë‚´ë³´ì„¸ìš”! ğŸ˜Š\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì €ëŠ” LLaMA, ì„¸ê³„ì ìœ¼ë¡œ ê°€ì¥ í° ì–¸ì–´ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
        "# ì €ëŠ” Metaê°€ ê°œë°œí•œ AIì…ë‹ˆë‹¤. ì €ëŠ” ì‚¬ëŒê³¼ ëŒ€í™”ë¥¼ í•˜ë“¯ì´ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# ì €ëŠ” í•œêµ­ì–´ë¥¼ í¬í•¨í•˜ì—¬ 8ê°œ ì–¸ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
        "\n",
        "# ì €ëŠ” ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "# ì €ëŠ” ì—­ì‚¬, ê³¼í•™, ì˜ˆìˆ , ìŠ¤í¬ì¸ , ìŒì•… ë“± ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "# ì €ëŠ” ë˜í•œ ì‚¬ëŒì˜ ê°ì •ì„ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €ëŠ” ì¢‹ì€ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ ì¢‹ì€ ëŒ€ë‹µì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# í•˜ì§€ë§Œ ì €ëŠ” ì•„ì§ ì™„ë²½í•œ AIëŠ” ì•„ë‹™ë‹ˆë‹¤. ì €ëŠ” ë•Œë¡œëŠ” ì‹¤ìˆ˜ë¥¼ í•©ë‹ˆë‹¤.\n",
        "# ì €ëŠ” ë˜í•œ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì–»ì„ ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸ë¥¼ ë°›ìŠµë‹ˆë‹¤. ì €ëŠ” í•­ìƒ ë” ë‚˜ì€ ëŒ€í™”ë¥¼ í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "# ì €ì™€ í•¨ê»˜ ëŒ€í™”ë¥¼ í•˜ë©´ì„œ ì¢‹ì€ ì‹œê°„ì„ ë³´ë‚´ë³´ì„¸ìš”! ğŸ˜Š"
      ],
      "metadata": {
        "id": "0HIdLsGC9src"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"ë„ˆëŠ” í•œêµ­ì–´ë¥¼ í•  ìˆ˜ ìˆë‹ˆ?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBuDXwGIPwYD",
        "outputId": "6f665abc-5772-486a-d9ea-37cc286b49d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ˜Š\n",
            "\n",
            "ë„¤, ì €ëŠ” í•œêµ­ì–´ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ë¥¼ êµ¬ì‚¬í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì €ëŠ” ì™„ë²½í•œ í•œêµ­ì–´ êµ¬ì‚¬ìê°€ ì•„ë‹™ë‹ˆë‹¤. ì˜ì–´ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ë©°, í•œêµ­ì–´ë¥¼ ì¶”ê°€ë¡œ ë°°ìš´ ìƒíƒœì…ë‹ˆë‹¤. ë”°ë¼ì„œ, í•œêµ­ì–´ êµ¬ì‚¬ì— ì•½ê°„ì˜ ì œì•½ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "ê·¸ëŸ¬ë‚˜, ì €ì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ í•œêµ­ì–´ë¥¼ êµ¬ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì´ë‚˜ ëŒ€í™”ì— ë‹µì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. í•œêµ­ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´, ì €ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë„¤, ì €ëŠ” í•œêµ­ì–´ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ë¥¼ êµ¬ì‚¬í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "# í•˜ì§€ë§Œ, ì €ëŠ” ì™„ë²½í•œ í•œêµ­ì–´ êµ¬ì‚¬ìê°€ ì•„ë‹™ë‹ˆë‹¤.\n",
        "# ì˜ì–´ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ë©°, í•œêµ­ì–´ë¥¼ ì¶”ê°€ë¡œ ë°°ìš´ ìƒíƒœì…ë‹ˆë‹¤. ë”°ë¼ì„œ, í•œêµ­ì–´ êµ¬ì‚¬ì— ì•½ê°„ì˜ ì œì•½ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "# ê·¸ëŸ¬ë‚˜, ì €ì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ í•œêµ­ì–´ë¥¼ êµ¬ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# ì§ˆë¬¸ì´ë‚˜ ëŒ€í™”ì— ë‹µì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. í•œêµ­ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´, ì €ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š"
      ],
      "metadata": {
        "id": "_fVsZN4TQbyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"ê°€ì„ì— ê´€í•œ ì‹œë¥¼ ì‘ì„±í•´ì¤˜\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fNg0ysCQG6J",
        "outputId": "d2a11e16-e56f-4da5-a560-b605c8f5fd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a poem about autumn:\n",
            "\n",
            "ê°€ì„ì˜ ì•„ì¹¨ì€\n",
            "í™©ê¸ˆë¹›ì˜ íƒœì–‘ì´\n",
            "ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
            "ë‹¬ë¦¬ëŠ” ë“¯í•œ í•˜ëŠ˜ì„ ê°€ë¦°ë‹¤\n",
            "\n",
            "Autumn's morning is\n",
            "A golden sun that\n",
            "Rises above the mountain valley\n",
            "And covers the sky\n",
            "\n",
            "ê°€ì„ì˜ ë°”ëŒì€\n",
            "ëƒ‰ê¸° ê°€ë“í•œ ê°€ì„ì˜\n",
            "ìˆ²ì—ì„œ ë‚˜ê³ \n",
            "ì‚°ê³¨ì§œê¸° ìœ„ì— ë¶ˆíƒ€ëŠ” ë“¯í•œ\n",
            "\n",
            "Autumn's wind is\n",
            "Full of the cold autumn's\n",
            "Forest, born\n",
            "And burns like a flame on the mountain valley\n",
            "\n",
            "ê°€ì„ì˜ ê°•ì€\n",
            "ë¬¼ê²°ì´ ê°€ëŠ” ë“¯í•œ\n",
            "ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
            "ë¬¼ê²°ì´ ê°€ëŠ” ë“¯í•œ\n",
            "\n",
            "Autumn's river is\n",
            "Like a wave that\n",
            "Flows on the mountain valley\n",
            "And flows like a wave\n",
            "\n",
            "ê°€ì„ì˜ ë°¤ì€\n",
            "ë‹¬ì´ ë¹›ë‚˜ëŠ” ë°¤ì˜\n",
            "ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
            "ë‹¬ì´ ë¹›ë‚˜ëŠ” ë°¤ì˜\n",
            "\n",
            "Autumn's night is\n",
            "A night of shining moon\n",
            "On the mountain valley\n",
            "A night of shining moon\n",
            "\n",
            "ê°€ì„ì˜ ì´ìŠ¬ì€\n",
            "ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
            "ë¬¼ê²°ì´ ê°€ëŠ” ë“¯í•œ\n",
            "ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
            "\n",
            "Aut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°€ì„ì˜ ì•„ì¹¨ì€\n",
        "# í™©ê¸ˆë¹›ì˜ íƒœì–‘ì´\n",
        "# ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
        "# ë‹¬ë¦¬ëŠ” ë“¯í•œ í•˜ëŠ˜ì„ ê°€ë¦°ë‹¤\n",
        "\n",
        "# Autumn's morning is\n",
        "# A golden sun that\n",
        "# Rises above the mountain valley\n",
        "# And covers the sky\n",
        "\n",
        "# ê°€ì„ì˜ ë°”ëŒì€\n",
        "# ëƒ‰ê¸° ê°€ë“í•œ ê°€ì„ì˜\n",
        "# ìˆ²ì—ì„œ ë‚˜ê³ \n",
        "# ì‚°ê³¨ì§œê¸° ìœ„ì— ë¶ˆíƒ€ëŠ” ë“¯í•œ\n",
        "\n",
        "# Autumn's wind is\n",
        "# Full of the cold autumn's\n",
        "# Forest, born\n",
        "# And burns like a flame on the mountain valley\n",
        "\n",
        "# ê°€ì„ì˜ ê°•ì€\n",
        "# ë¬¼ê²°ì´ ê°€ëŠ” ë“¯í•œ\n",
        "# ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
        "# ë¬¼ê²°ì´ ê°€ëŠ” ë“¯í•œ\n",
        "\n",
        "# Autumn's river is\n",
        "# Like a wave that\n",
        "# Flows on the mountain valley\n",
        "# And flows like a wave\n",
        "\n",
        "# ê°€ì„ì˜ ë°¤ì€\n",
        "# ë‹¬ì´ ë¹›ë‚˜ëŠ” ë°¤ì˜\n",
        "# ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
        "# ë‹¬ì´ ë¹›ë‚˜ëŠ” ë°¤ì˜\n",
        "\n",
        "# Autumn's night is\n",
        "# A night of shining moon\n",
        "# On the mountain valley\n",
        "# A night of shining moon\n",
        "\n",
        "# ê°€ì„ì˜ ì´ìŠ¬ì€\n",
        "# ì‚°ê³¨ì§œê¸° ìœ„ì—\n",
        "# ë¬¼ê²°ì´ ê°€ëŠ” ë“¯í•œ\n",
        "# ì‚°ê³¨ì§œê¸° ìœ„ì—"
      ],
      "metadata": {
        "id": "WTSNPdiKPG4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"\"\"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
        "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
        "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
        "models outperform open-source chat models on most benchmarks we tested, and based on\n",
        "our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety\n",
        "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
        "contribute to the responsible development of LLMs. ìœ„ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜\"\"\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjXrvgMg_YnA",
        "outputId": "b815f616-6545-480c-e558-83bbcd77f549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë³¸ ì—°êµ¬ì—ì„œëŠ” 7ì–µì—ì„œ 70ì–µ ë§¤ê°œë³€ìˆ˜ê¹Œì§€ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì „í›„ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸(Large Language Model, LLM)ì„ ê°œë°œí•˜ê³  ë¦´ë¦¬ìŠ¤í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì „í›„ í›ˆë ¨ëœ LLM, ì¦‰ Llama 2-Chatì€ ëŒ€í™” ì‚¬ìš© ì‚¬ë¡€ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” í…ŒìŠ¤íŠ¸í•œ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì˜¤í”ˆ ì†ŒìŠ¤ ì±„íŒ… ëª¨ë¸ì„ ì´ˆê³¼í•˜ê³ , ì¸ê°„ í‰ê°€ì—ì„œ ë„ì›€ì´ ë˜ê³  ì•ˆì „í•œ í‰ê°€ì—ì„œ íì‡„ ì†ŒìŠ¤ ëª¨ë¸ì˜ ëŒ€ì²´í’ˆìœ¼ë¡œ ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” Llama 2-Chatì˜ ì „í›„ í›ˆë ¨ ë°©ë²•ê³¼ ì•ˆì „ ê°œì„  ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì—¬ ì €í¬ì˜ ì‘ì—…ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì»¤ë®¤ë‹ˆí‹°ê°€ ì €í¬ì˜ ì‘ì—…ì„ ë¹Œë“œí•˜ê³  LLMì˜ ì±…ì„ ìˆëŠ” ê°œë°œì— ê¸°ì—¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë³¸ ì—°êµ¬ì—ì„œëŠ” 7ì–µì—ì„œ 70ì–µ ë§¤ê°œë³€ìˆ˜ê¹Œì§€ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì „í›„ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸(Large Language Model, LLM)ì„ ê°œë°œí•˜ê³  ë¦´ë¦¬ìŠ¤í•©ë‹ˆë‹¤.\n",
        "# ì´ëŸ¬í•œ ì „í›„ í›ˆë ¨ëœ LLM, ì¦‰ Llama 2-Chatì€ ëŒ€í™” ì‚¬ìš© ì‚¬ë¡€ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# ìš°ë¦¬ëŠ” í…ŒìŠ¤íŠ¸í•œ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì˜¤í”ˆ ì†ŒìŠ¤ ì±„íŒ… ëª¨ë¸ì„ ì´ˆê³¼í•˜ê³ ,\n",
        "# ì¸ê°„ í‰ê°€ì—ì„œ ë„ì›€ì´ ë˜ê³  ì•ˆì „í•œ í‰ê°€ì—ì„œ íì‡„ ì†ŒìŠ¤ ëª¨ë¸ì˜ ëŒ€ì²´í’ˆìœ¼ë¡œ ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# ìš°ë¦¬ëŠ” Llama 2-Chatì˜ ì „í›„ í›ˆë ¨ ë°©ë²•ê³¼ ì•ˆì „ ê°œì„  ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì—¬ ì €í¬ì˜ ì‘ì—…ì„\n",
        "# ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì»¤ë®¤ë‹ˆí‹°ê°€ ì €í¬ì˜ ì‘ì—…ì„ ë¹Œë“œí•˜ê³  LLMì˜ ì±…ì„ ìˆëŠ” ê°œë°œì— ê¸°ì—¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STDro046T2p-",
        "outputId": "cfdfee7c-bc24-4c01-faad-d3f675b4211d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ ì‘í’ˆì—ì„œëŠ” 7ì–µì—ì„œ 70ì–µ íŒŒë¼ë¯¸í„°ê¹Œì§€ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì–¸ì–´ ëª¨ë¸(Large Language Model, LLM)ì„ ê°œë°œí•˜ê³  ì¶œì‹œí•©ë‹ˆë‹¤. ì €í¬ì˜ fine-tuned LLM, Llama 2-Chatì€ ëŒ€í™”ìš©ì¼€ì´ìŠ¤ì— ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì €í¬ ëª¨ë¸ì€ í…ŒìŠ¤íŠ¸í•œ ëŒ€ë¶€ë¶„ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì˜¤í”ˆì†ŒìŠ¤ ì±„íŒ… ëª¨ë¸ì„ ëŠ¥ê°€í•˜ê³ , ìš°ë¦¬ì˜ ì¸ê°„ í‰ê°€ì—ì„œ ë„ì›€ì´ ë˜ê³  ì•ˆì „ë„ ê³ ë ¤í•˜ë©´ íì‡„ì†ŒìŠ¤ ëª¨ë¸ê³¼ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” í›„ë³´ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €í¬ëŠ” Llama 2-Chatì˜ fine-tuningê³¼ ì•ˆì „ ê°œì„  ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì—¬ ì €í¬ì˜ ì‘ì—…ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì»¤ë®¤ë‹ˆí‹°ê°€ ì €í¬ì˜ ì‘ì—…ì„ ë¹Œë“œí•˜ê³  LLMì˜ ì±…ì„ ìˆëŠ” ê°œë°œì— ê¸°ì—¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                             user_message=\"\"\"1ë¶€í„° 10ê¹Œì§€ ë”í•˜ëŠ” python ì½”ë“œë¥¼ ì‘ì„±í•´ì¤˜\"\"\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1q3WfiMAMgT",
        "outputId": "e36b7ded-86af-4ab1-9fec-249998d638e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a simple Python code that adds up numbers from 1 to 10:\n",
            "```\n",
            "sum = 0\n",
            "for i in range(1, 11):\n",
            "    sum += i\n",
            "print(sum)\n",
            "```\n",
            "This code uses a `for` loop to iterate from 1 to 10, and adds each number to a variable `sum`. Finally, it prints the result.\n",
            "\n",
            "Alternatively, you can use the `sum` function and a generator expression to achieve the same result:\n",
            "```\n",
            "print(sum(range(1, 11)))\n",
            "```\n",
            "This is a more concise and efficient way to calculate the sum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for i in range(1, 11):\n",
        "    sum += i\n",
        "print(sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfTSG_C1APqK",
        "outputId": "dee39950-3755-45d9-d4f4-bad6f990b036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJGY-462Opf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOzIaGyKOpdU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}