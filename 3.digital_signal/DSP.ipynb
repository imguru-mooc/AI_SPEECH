{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU2aKMwT21Oq"
      },
      "source": [
        "## 1. Digital Signal Processing\n",
        "\n",
        "저희의 목적은 소리 signal를 어떠한 데이터 타입으로 표현하며, 소리와 관련된 task를 해결하는데 있습니다. 그렇다면 소리는 어떠한 데이터를 가지고 있을까요?\n",
        "\n",
        "### Sound?\n",
        "\n",
        "소리는 일반적으로 진동으로 인한 공기의 압축으로 생성됩니다. 그렇다면 압축이 얼마나 됬느냐에 따라서 표현되것이 바로 Wave(파동)인데요. 파동은 진동하며 공간/매질을 전파해 나가는 현상입니다. 질량의 이동은 없지만 에너지/운동량의 운반은 존재합니다.\n",
        "\n",
        "Wave에서 저희가 얻을수 있는 정보는 크게 3가지가 있습니다.\n",
        "- Phase(Degress of displacement) : 위상\n",
        "- Amplitude(Intensity) : 진폭\n",
        "- Frequency : 주파수\n",
        "\n",
        "\n",
        "### Sampling\n",
        "\n",
        "샘플링은 무엇일까요?? 아날로그 정보를 잘게 쪼개서 discrete한 디지털 정보로 표현해야합니다. 우리는 무한하게 쪼개서 저장할수 없으니, 어떤 기준을 가지고 아날로그 정보를 쪼개서 대표값을 취하게 됩니다.\n",
        "\n",
        "```Convert into a sqeuence of binary values via Sampling and Quantization```\n",
        "\n",
        "### 1.1. Time domain\n",
        "\n",
        "시간을 기준으로 아날로그 시그널을 쪼개게 되는 것을 의미합니다. Sampling을 통하여 컴퓨터는 소리 sequence를 binary value로 받아드리게 됩니다.\n",
        "\n",
        "__Sampling rate : 얼마나 잘게 쪼갤 것인가?__<br>\n",
        "잘개 쪼갤수록 원본 데이터와 거이 가까워지기 떄문에 좋지만 Data의 양이 증가하게 됩니다. 만약 너무 크게 쪼개게 된다면, 원본 데이터로 reconstruct하기 힘들어 질 것입니다.\n",
        "\n",
        "\n",
        "\n",
        "__Sampling theorem__<br>\n",
        "샘플링 레이트가 최대 frequency의 2배 보다 커져야 한다는 것입니다.\n",
        "$ f_{s} > 2f_{m} $ 여기서 $f_{s}$는 sampling rate, 그리고 $f_{m}$은 maximum frequency를 말합니다.\n",
        "\n",
        "- Nyquist frequency = $f_{s}/2$, sampling rate의 절반입니다.\n",
        "\n",
        "일반적으로 Sampling은 인간의 청각 영역에 맞게 형성이 됩니다.\n",
        "- Audio CD : 44.1 kHz(44100 sample/second)\n",
        "- Speech communication : 8 kHz(8000 sample/second)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4hfYNb2wu9m"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb06mH8jxGtr"
      },
      "outputs": [],
      "source": [
        "# tra\n",
        "# in_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI4LaY7245Pm"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXqjGSAvh7sZ"
      },
      "outputs": [],
      "source": [
        "type(test_dataset.get_metadata)\n",
        "test_dataset.get_metadata(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8JvgqogxgtA"
      },
      "outputs": [],
      "source": [
        "test_dataset[0]\n",
        "\n",
        "#소리 데이터, 샘플레이트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9v2gDQAoOw4"
      },
      "outputs": [],
      "source": [
        "audioData = test_dataset[0][0][0]\n",
        "sr = test_dataset[0][1]\n",
        "print(len(audioData), sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nQ7xfQh2MPT"
      },
      "outputs": [],
      "source": [
        "len(audioData) / sr #duration이 나옴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYj4nOm42cQJ"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio(audioData, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ-th5zk3QKt"
      },
      "source": [
        "\n",
        "### Resampling\n",
        "\n",
        "샘플링된 데이터를 다시금 더 높은 sampling rate 혹은 더 낮은 sampling rate로 다시 샘플링할수 있습니다. 이때는 일반적으로 interpolation(보간)을 할때는 low-pass filter를 사용합니다.(Windowed sinc function)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC0Bdlyq2r_P"
      },
      "outputs": [],
      "source": [
        "y_8k = librosa.resample(audioData.numpy(), orig_sr=sr, target_sr=8000) #torch에서 바로 import 해서 tensor로 되어 있음 -> numpy 붙이기\n",
        "ipd.Audio(y_8k, rate=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCLki6Ed4Skp"
      },
      "outputs": [],
      "source": [
        "len(y_8k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5C1G5l73xRZ"
      },
      "outputs": [],
      "source": [
        "y_4k = librosa.resample(audioData.numpy(), orig_sr=sr, target_sr=4000)\n",
        "ipd.Audio(y_4k, rate=4000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYTeTAlB4WRE"
      },
      "outputs": [],
      "source": [
        "len(y_4k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nWoD9Px4h-b"
      },
      "source": [
        "### Nomalization & Quantization\n",
        "\n",
        "시간의 기준이 아닌 실제 amplitude의 real valued 를 기준으로 시그널의 값을 조절합니다. Amplitude를 이산적인 구간으로 나누고, signal 데이터의 Amplitude를 반올림하게 됩니다.\n",
        "\n",
        "그렇다면 이산적인 구간은 어떻게 나눌수 있을까요?, bit의 비트에 의해서 결정됩니다.\n",
        "- B bit의 Quantization : $-2^{B-1}$ ~ $2^{B-1}-1$\n",
        "- Audio CD의 Quantization (16 bits) : $-2^{15}$ ~ $2^{15}-1$\n",
        "- 위 값들은 보통 -1.0 ~ 1.0  영역으로 scaling되기도 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13Vnyl8qGfJu"
      },
      "outputs": [],
      "source": [
        "audio_np = audioData.numpy()\n",
        "print(max(np.abs(audio_np)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyGSDrqw4VkL"
      },
      "outputs": [],
      "source": [
        "audio_np = audioData.numpy()\n",
        "normed_wav = audio_np / max(np.abs(audio_np))\n",
        "print(normed_wav, min(normed_wav), max(normed_wav))\n",
        "ipd.Audio(normed_wav, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzL74Zxk43fK"
      },
      "outputs": [],
      "source": [
        "#quantization 하면 음질은 떨어지지만 light한 자료형이 된다.\n",
        "Bit = 16\n",
        "max_value = 2 ** (Bit-1)\n",
        "\n",
        "quantized_8_wav = normed_wav * max_value\n",
        "print(quantized_8_wav)\n",
        "quantized_8_wav = np.round(quantized_8_wav).astype(int)\n",
        "quantized_8_wav = np.clip(quantized_8_wav, -max_value, max_value-1)\n",
        "ipd.Audio(quantized_8_wav, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76wdBYnL5n4m"
      },
      "source": [
        "### mu-law encoding\n",
        "\n",
        "사람의 귀는 소리의 amplitude에 대해 log적으로 반응합니다. 즉, 작은소리의 차이는 잘잡아내는데 반해 소리가 커질수록 그 차이를 잘 느끼지 못합니다. 이러한 특성을 wave값을 표현하는데 반영해서 작은값에는 높은 분별력(high resolution)을, 큰값끼리는 낮은 분별력(low resolution)을 갖도록 합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EOHWa0g5Fah"
      },
      "outputs": [],
      "source": [
        "def mu_law(x, mu=255):\n",
        "    return np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjCPmLYKQEZM"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-1, 1, 1000)\n",
        "x_mu = mu_law(x)\n",
        "\n",
        "plt.figure(figsize=[6, 4])\n",
        "plt.plot(x)\n",
        "plt.plot(x_mu)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkaYw3Gu5vKd"
      },
      "outputs": [],
      "source": [
        "wav_mulaw = mu_law(normed_wav)\n",
        "ipd.Audio(wav_mulaw, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZjuV0d-6dJ_"
      },
      "source": [
        "## 2. Sound Representation\n",
        "\n",
        "위에서 Sampling된 discrete한 데이터를 이제 우리는 표현이 가능합니다. 그렇다면 어떤 요소를 기반으로 저희가 데이터를 표현해야할까요?, 첫번째는 시간의 흐름에 따라, 공기의 파동의 크기로 보는 Time-domain Representation 방법이 있습니다. 두번째는 시간에 따라서 frequency의 변화를 보는 Time-Frequency representation이 있습니다.\n",
        "\n",
        "### 2.1. Time domain - Waveform\n",
        "\n",
        "Waveform의 경우에는 오디오의 자연적이 표현입니다. 시간이 x축으로 그리고 amplitude가 y축으로 표현이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dncYzgiQ5y4h"
      },
      "outputs": [],
      "source": [
        "import librosa.display\n",
        "\n",
        "audio_np = audioData.numpy() #-> numpy로 바꾸기\n",
        "\n",
        "fig = plt.figure(figsize = (14,5))\n",
        "librosa.display.waveshow(audio_np[:], sr=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNGk1a2L6ze0"
      },
      "source": [
        "### 정현파 (Sinusoid)\n",
        "\n",
        "모든 신호는 주파수(frequency)와 크기(magnitude), 위상(phase)이 다른 정현파(sinusolida signal)의 조합으로 나타낼 수 있다. 퓨리에 변환은 조합된 정현파의 합(하모니) 신호에서 그 신호를 구성하는 정현파들을 각각 분리해내는 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPxW98IF6lB3"
      },
      "outputs": [],
      "source": [
        "A = 0.9\n",
        "f = 262 # 계이름 '도'\n",
        "phi = np.pi/2\n",
        "fs = 22050\n",
        "t = 1\n",
        "#여러개의 정현파를 합치면 소리의 신호. 반대로 소리에서 정현파를 분리할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-rurYCpLkp2"
      },
      "outputs": [],
      "source": [
        "temp = np.arange(0,t,1.0/fs)\n",
        "print(temp, len(temp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGyQHhmE6_rL"
      },
      "outputs": [],
      "source": [
        "def Sinusoid(A,f,phi,fs,t):\n",
        "    t = np.arange(0,t,1.0/fs)\n",
        "    # x = A * np.cos(2*np.pi*f*t+phi)\n",
        "    x = A * np.sin(2*np.pi*f*t)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPItR6DsgAqB"
      },
      "outputs": [],
      "source": [
        "sin_c = Sinusoid(A,f,phi,fs,t)\n",
        "# print(sin_c[:85])\n",
        "plt.plot(sin_c[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNoJBfW-knZy"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (14,5))\n",
        "librosa.display.waveshow(sin_c[:100], sr=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYTs0TP-NPLe"
      },
      "outputs": [],
      "source": [
        "ipd.Audio(sin_c, rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMWI2woWN_g-"
      },
      "outputs": [],
      "source": [
        "f = 330 # 계이름 '미'\n",
        "sin_e = Sinusoid(A,f,phi,fs,t)\n",
        "plt.plot(sin_e[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5fDkJjvN011"
      },
      "outputs": [],
      "source": [
        "f = 392# 계이름 '솔'\n",
        "sin_g = Sinusoid(A,f,phi,fs,t)\n",
        "plt.plot(sin_g[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KEw-h8tgOco"
      },
      "outputs": [],
      "source": [
        "complexwave = sin_c  + sin_e + sin_g\n",
        "plt.plot(complexwave[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2RtSMHk7A6o"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (14,5))\n",
        "librosa.display.waveshow(complexwave[:1000], sr=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBAIn2IN7B-w"
      },
      "outputs": [],
      "source": [
        "ipd.Audio(complexwave, rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE4XHSUsVc8P"
      },
      "outputs": [],
      "source": [
        "def generate_sinusoid_scale(pitches=[69], duration=0.5, Fs=22050, amplitude_max = 0.5):\n",
        "    N = int(duration * Fs)\n",
        "    t = np.arange(N) / Fs\n",
        "    x = []\n",
        "    for p in pitches:\n",
        "        omega = 2 ** (( p - 69 ) / 12 ) * 440\n",
        "        x = np.append(x, np.sin(2 * np.pi * omega * t))\n",
        "    x = amplitude_max * x/np.max(x)\n",
        "    return x\n",
        "\n",
        "duration = 1\n",
        "Fs = 22050\n",
        "\n",
        "pitches = [36,48,60,72,84,96]\n",
        "x = generate_sinusoid_scale(pitches=pitches, duration=duration, Fs=Fs)\n",
        "print('Pitch class C = {..., C1, C2, C3, C4, C5, C6, ...}', flush=True)\n",
        "ipd.display(ipd.Audio(data=x, rate=Fs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bZCXCwW7NxR"
      },
      "source": [
        "### 푸리에 변환 (Fourier transform)\n",
        "\n",
        "\n",
        "푸리에 변환(Fourier transform)을 직관적으로 설명하면 푸리에 변환은 임의의 입력 신호를 다양한 주파수를 갖는 주기함수(복수 지수함수)들의 합으로 분해하여 표현하는 것 입니다. 그리고 각 주기함수들의 진폭을 구하는 과정을 퓨리에 변환이라고 합니다.\n",
        "\n",
        "- 주기(period): 파동이 한번 진동하는데 걸리는 시간, 또는 그 길이, 일반적으로 sin함수의 주기는 $2\\pi /w$입니다\n",
        "- 주파수(frequency): 1초동안의 진동횟수입니다.\n",
        "\n",
        "퓨리에 변환의 식을 살펴봅시다.\n",
        "\n",
        "$$\n",
        "y(t)=\\sum_{k=-\\infty}^\\infty A_k \\, \\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right)\n",
        "$$\n",
        "\n",
        "이 식을 하나식 해석해봅시다. $k$는 $-\\infty ~ \\infty$의 범위를 가지고 움직입니다. 이것은 주기함수들의 갯수입니다. 어떠한 신호가 다른 주기함수들의 합으로 표현되는데, 그 주기함수는 무한대의 범위에 있군요.\n",
        "\n",
        "그렇다면 $A_k$은 그 사인함수의 진폭이라고 합니다. 이 식은 시간에 대한 입력신호 $y_{t}$가  $\\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right)$와 진폭($A_k$)의 선형결합으로 표현됨을 말하고 있군요.\n",
        "\n",
        "\n",
        "위 그림을 본다면 조금 더 명확히 알수 있을 것 같습니다. 붉은색 라인이 입력신호 $y_{t}$ 입니다. 일반적으로 우리가 다루게 되는 데이터인 음악이나 목소리 같은 데이터 역시 complex tone입니다. 여려개의 주파수영역이 합쳐진 것이죠. 이러한 여러개의 주파수 영역을 분리하자!가 주요한 아이디어입니다. 파란색 주기함수들을 보신다면 여러개의 주기함수들을 찾으실 수 있습니다. 그 주기함수들은 고유의 주파수(frequency)와 강도(amplitude)를 가지고 있고 그것이 파란색의 라인들로 표현되어 있습니다.\n",
        "\n",
        "진폭에 대한 수식은 다음과 같습니다.\n",
        "\n",
        "$$\n",
        "A_k = \\frac{1}{T} \\int_{-\\frac{T}{2}}^\\frac{T}{2} f(t) \\, \\exp \\left( -i\\cdot 2\\pi \\frac{k}{T} t \\right) \\, dt\n",
        "$$\n",
        "\n",
        "여기서 하나의 의문점이 드실것 같습니다. 주기함수의 합으로 표현된다고 했는데 저희가 보고 있는것은 $\\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right)$ 지수함수의 형태이기 때문입니다.\n",
        "\n",
        "지수함수와 주기함수 사이의 연관관계는 무엇일까요? 그 관계를 찾은 것이 바로 오일러 공식입니다.\n",
        "\n",
        "$$\n",
        "e^{i\\theta} = \\cos{\\theta} + i\\sin{\\theta}\n",
        "$$\n",
        "\n",
        "이 식을 위 식처럼 표현한다면 다음과 같습니다\n",
        "$$\n",
        "\\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right) = \\cos\\left({2\\pi\\frac{k}{T}}\\right) + i\\sin\\left({2\\pi\\frac{k}{T}}\\right)\n",
        "$$\n",
        "\n",
        "여기서 $\\cos{2\\pi\\frac{k}{T}}$, $i\\sin{2\\pi\\frac{k}{T}}$ 함수는 주기와 주파수를 가지는 주기함수입니다.\n",
        "\n",
        "즉 퓨리에 변환은 입력 singal이 어떤것인지 상관없이 sin, cos과 같은 주기함수들의 합으로 항상 분해 가능하다는 것입니다.\n",
        "\n",
        "### Fourier Transform의 Orthogonal\n",
        "\n",
        "$$\n",
        "y(t)=\\sum_{k=-\\infty}^\\infty A_k \\, \\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right)\n",
        "$$\n",
        "\n",
        "어떠한 주기함수를 우리는 cos과 sin함수로 표현하게 되었습니다. 여기서 한가지 재밌는 점은, 이 함수들이 직교하는 함수(orthogonal)라는 점이다.\n",
        "$$\n",
        "\\{ \\exp \\left(i\\cdot 2\\pi\\frac{k}{T} t\\right) \\} = orthogonal\n",
        "$$\n",
        "\n",
        "벡터의 직교는 해당 벡터를 통해 평면의 모든 좌표를 표현할수 있었다. 함수의 내적은 적분으로 표현할 수 있는데, 만약 구간 [a,b]에서 직교하는 함수는 구간 [a,b]의 모든 함수를 표현할수 있습니다.\n",
        "\n",
        "위 케이스에서는 cos, sin 함수가 사실상 우리 입력신호에 대해서 기저가 되어주는 함수라고 생각할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98US5knu-O-I"
      },
      "source": [
        "[e 상수 동영상](https://youtu.be/49ntos1ePS8)     \n",
        "[퓨리에 변환 동영상](https://youtu.be/Mc9PHZ3H36M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ6d2pYi7Vut"
      },
      "source": [
        "### DFT (Discrete Fourier Transform)\n",
        "\n",
        "한가지 의문점이 듭니다. 바로, 우리가 sampling으로 들어온 데이터는 바로 시간의 간격에 따른 소리의 amplitude의 discrete한 데이터이기 때문이다. 그렇다면 위 푸리에 변환 식을 Discrete한 영역으로 생각해봅시다.\n",
        "\n",
        "만약에 우리가 수집한 데이터 $y_{n}$에서, 이산 시계열 데이터가 주기 $N$으로 반복한다고 할때, DFT는 주파수와 진폭이 다른 $N$개의 사인 함수의 합으로 표현이 가능합니다.\n",
        "$$\n",
        "y_n = \\frac{1}{N} \\sum_{k=0}^{N-1} Y_k \\cdot \\exp \\left( i\\cdot 2\\pi\\frac{k}{N} n \\right)\n",
        "$$\n",
        "\n",
        "위 식을 보면 k의 range가 0부터 $N-1$로 변화했음을 알 수 있다. 이때 Spectrum $Y_{k}$를 원래의 시계열 데이터에 대한 퓨리에 변환값이라고 하죠.\n",
        "\n",
        "$$\n",
        "Y_k = \\sum_{n=0}^{N-1} y_n\\cdot \\exp \\left( -i\\cdot 2\\pi\\frac{k}{N} n \\right)\n",
        "$$\n",
        "\n",
        "- $y_{n}$ : input signal\n",
        "- $n$ : Discrete time index\n",
        "- $k$ : discrete frequency index\n",
        "- $Y_{k}$ : k번째 frequeny에 대한 Spectrum의 값"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8YBEf1w7I7H"
      },
      "outputs": [],
      "source": [
        "def DFT(x):\n",
        "    N = len(x)\n",
        "    X = np.array([])\n",
        "    nv = np.arange(N)\n",
        "\n",
        "    for k in range(N):\n",
        "        s = np.exp(1j*2*np.pi*k/N*nv)\n",
        "        X = np.append(X, sum(x*np.conjugate(s)))\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQDtzOtlEvW9"
      },
      "outputs": [],
      "source": [
        "N = 1000\n",
        "k0 = 100\n",
        "nv = np.arange(N)\n",
        "x = np.exp(1j*2*np.pi*k0/N*nv)\n",
        "\n",
        "k1 = 200\n",
        "x += np.exp(1j*2*np.pi*k1/N*nv)\n",
        "\n",
        "k2 = 300\n",
        "x += np.exp(1j*2*np.pi*k2/N*nv)\n",
        "\n",
        "X = DFT(x)\n",
        "plt.plot(abs(X))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-0QClRN8bww"
      },
      "source": [
        "### STFT (Short-Time Fourier Transform)\n",
        "\n",
        "FFT는 시간에 흐름에 따라 신호의 수파수가 변했을때, 어느 시간대에 주파수가 변하는지 모르게 됩니다. 이러한 한계를 극복하기 위해서, STFT는 시간의 길이를 나눠서 이제 퓨리에 변환을 하게 됩니다. 즉 FFT를 했을때는 Time domina에 대한 정보가 날아가게 되는 것이죠.\n",
        "\n",
        "주파수의 특성이 시간에 따라 달라지는 사운드를 분석하는 방법입니다. 일반적으로 우리가 사용하는 signal 데이터에 적합하다. 시계열 데이터를 일정한 시간 구간 (window size)로 나누고, 각 구간에 대해서 스펙트럼을 구하는 데이터이다. 이는 Time-frequency 2차원 데이터로 표현이 됩니다.\n",
        "\n",
        "$$\n",
        "X(l,k) = \\sum_{n=0}^{N-1} w(n) x(n+lH)\\exp^{\\frac{-2\\pi k n}{N}}\n",
        "$$\n",
        "\n",
        "- $N$ : FFT size\n",
        "    - Window를 얼마나 많은 주파수 밴드로 나누는가 입니다.\n",
        "\n",
        "- Duration\n",
        "    - 샘플링 레이트를 window로 나눈 값입니다.\n",
        "    - $T= window/SR$\n",
        "    - T(Window) = 5T(Signal), duration은 신호주기보다 5배 이상 길게 잡아야한다.\n",
        "    - 440Hz 신호의 window size는 5*(1/440)이 됩니다.\n",
        "\n",
        "- $w(n)$ : Window function\n",
        "    - 일반적으로 Hann window가 쓰입니다.\n",
        "\n",
        "- $n$ : Window size\n",
        "    - Window 함수에 들어가는 Sample의 양입니다.\n",
        "    - 작을수록 Low-frequency resolution을 가지게 되고, high-time resolution을 가집니다.\n",
        "    - 길수록 High-frequency, low time resolution을 가집니다.\n",
        "    \n",
        "- $H$ : Hop size\n",
        "    - 윈도우가 겹치는 사이즈입니다. 일반적으로는 1/4정도를 겹치게 합니다.\n",
        "\n",
        "STFT의 결과는 즉 시간의 흐름(Window)에 따른 Frequency영역별 Amplitude를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XQYMt43p94f"
      },
      "outputs": [],
      "source": [
        "audioData = test_dataset[1][0][0]\n",
        "sr = test_dataset[1][1]\n",
        "print(audioData, audioData.shape)\n",
        "audio_np = audioData.numpy()\n",
        "print(audio_np, len(audio_np))\n",
        "print(sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15inl95E8eRC"
      },
      "outputs": [],
      "source": [
        "# STFT\n",
        "S = librosa.core.stft(audio_np, n_fft=1024, hop_length=512, win_length=1024)\n",
        "S.shape, len(S[0]), S[0][0]\n",
        "#input shape 중요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_0rjNjbqYUs"
      },
      "outputs": [],
      "source": [
        "S"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPaStxQOqIaI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2FGhK-s8kBw"
      },
      "outputs": [],
      "source": [
        "# phase 에 대한 정보를 날린다.\n",
        "D = np.abs(S)**2\n",
        "D.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH6o-UEFqVeR"
      },
      "outputs": [],
      "source": [
        "D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY8Jj7Nvr0GW"
      },
      "outputs": [],
      "source": [
        "S = librosa.core.stft(audio_np, n_fft=1024, hop_length=512, win_length=1024)\n",
        "D = np.abs(S)**2\n",
        "log_S = librosa.power_to_db(S, ref=np.max) #소리의 단위를 db로 바꿈\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(log_S, sr=16000, x_axis='time')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyxFnyH98pge"
      },
      "source": [
        "### Window function?\n",
        "\n",
        "위에서 Window function과 Window size라는 이야기가 나오고 있습니다. 윈도우 Function과 Size는 왜 쓰는 것이며 어떨때 쓰는 것일까요?\n",
        "\n",
        "Window function의 주된 기능은 main-lobe의 width와 side-lobe의 레벨의 Trade-off 를 제어해 준다는 장점이 있습니다. 그리고 깁스 현상을 막아주는 고마운 친구이기도 하죠. 지금나온 main-lobe, side-bloe, 깁스현상은 무엇일까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKIGV9bufAkV"
      },
      "outputs": [],
      "source": [
        "a = np.arange(1,11)\n",
        "print(a)\n",
        "# b = np.pad(a, 3 )\n",
        "b = np.pad(a, 3, mode='reflect' )\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjgMLTT7I0W3"
      },
      "outputs": [],
      "source": [
        "np.round(1.45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIdiopUN8vZo"
      },
      "outputs": [],
      "source": [
        "def frame_audio(audio, FFT_size=1024, hop_size=20, sample_rate = 22050):\n",
        "    audio = np.pad(audio, int(FFT_size/2), mode='reflect')   #  512+52400+512\n",
        "    print(audio.shape)\n",
        "    frame_len = np.round(sample_rate*hop_size / 1000).astype(int)\n",
        "    print(frame_len)                                         #  320\n",
        "    frame_num = int((len(audio) - FFT_size) / frame_len) + 1\n",
        "    print(frame_num)                                         #  164\n",
        "    frames = np.zeros((frame_num, FFT_size))\n",
        "    print(frames.shape)                                      # (164,1024)\n",
        "\n",
        "    for n in range(frame_num):\n",
        "        frames[n] = audio[n*frame_len:n*frame_len+FFT_size]\n",
        "    return frames\n",
        "\n",
        "print(audio_np.shape)\n",
        "audio_framed = frame_audio(audio_np, sample_rate = sr)\n",
        "# print(\"Framed audio shape: {}\".format(audio_framed.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7E1ldcB8n2Z"
      },
      "outputs": [],
      "source": [
        "from scipy import signal\n",
        "\n",
        "window = signal.get_window(\"hann\", 1024, fftbins=True)\n",
        "audio_win = audio_framed * window\n",
        "ind = 2\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(window)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot(audio_framed[ind])\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.plot(audio_win[ind])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KODohbJ682iQ"
      },
      "source": [
        "플롯을 보게 된다면 windowing을 적용하기전 plot은 끝부분이 다 다르지만, windowing을 지나고 나서 나오는 plot은 끝이 0 으로 일치한다는 특성을 볼 수 있습니다.\n",
        "\n",
        "### Window size?\n",
        "\n",
        "윈도우 사이즈는 일반적으로 time과 frequency의 resolutions을 제어해 줍니다.\n",
        "- short-window : 낮은 frequency resolution, 높은 time-resolution을 가지게 됩니다.\n",
        "- Long-window : 높은 frequency resolution을 가지며, 낮은 time-resolution을 가지게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uy6gLXb86dP"
      },
      "source": [
        "### Spectrogram\n",
        "Spectrogram을 추출하는 방법을 고민해봅시다.일반적으로 프로세스는 입력신호에 대해서 window function을 통과하여 window size만큼 sampling 된 data를 받아서 Discrete Fourier Transform을 거치게 됩니다. DFT를 거친 신호들은 Frequency와 Amplitude의 영역을 가지는 Spectrum이 됩니다. 이후 이를 90도로 회전시켜서, time domain으로 stack하게 됩니다.\n",
        "\n",
        "Spectrogram은 Frequency Scale에 대해서 Scaling이 이루어집니다. 주파수 영역에 Scaling을 하는 이유는, 인간의 주파수를 인식하는 방식과 연관이 있습니다.\n",
        "\n",
        "일반적으로 사람은, 인접한 주파수를 크게 구별하지 못합니다. 그 이유는 우리의 인지기관이 categorical한 구분을 하기 때문입니다. 때문에 우리는 주파수들의 Bin의 그룹을 만들고 이들을 합하는 방식으로, 주파수 영역에서 얼마만큼의 에너지가 있는지를 찾아볼 것입니다. 일반적으로는 인간이 적은 주파수에 더 풍부한 정보를 사용하기때문에, 주파수가 올라갈수록 필터의 폭이 높아지면서 고주파는 거의 고려를 안하게 됩니다.\n",
        "\n",
        "따라서 아래 frequency scale은 어떤 방식을 통해 저주파수대 영역을 고려할 것이가에 대한 고민이 남아 있습니다.\n",
        "\n",
        "### Linear frequency scale\n",
        "\n",
        "일반적으로 single tone(순음)들의 배음 구조를 파악하기 좋습니다. 하지만 분포가 저주파수 영역에 기울어져(skewed) 있습니다.\n",
        "\n",
        "### Mel Scale\n",
        "멜 스펙트럼은 주파수 단위를 다음 공식에 따라 멜 단위로 바꾼 것을 의미합니다.\n",
        "\n",
        "$$\n",
        "m = 2595 \\log_{10}\\left(1 + \\frac{f}{700}\\right)\n",
        "$$\n",
        "일반적으로는 mel-scaled bin을 FFT size보다 조금더 작게 만드는게 일반적입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llENJknsyMsz"
      },
      "outputs": [],
      "source": [
        "f = np.arange(20000)\n",
        "m = 2595*np.log10(1+f/700)\n",
        "plt.plot(f, m)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFSvuL5_8sT5"
      },
      "outputs": [],
      "source": [
        "# STFT\n",
        "print(\"audio_np\", audio_np.shape, audio_np[:100])\n",
        "S = librosa.core.stft(audio_np, n_fft=1024, hop_length=512, win_length=1024)\n",
        "print(\"S\", S.shape, S[:100])\n",
        "# phase 에 대한 정보를 날린다.\n",
        "D = np.abs(S)**2\n",
        "print(\"D\", D.shape, D[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FitZqZtNVMsm"
      },
      "outputs": [],
      "source": [
        "# mel spectrogram (512 --> 40)\n",
        "mel_basis = librosa.filters.mel(sr=sr, n_fft=1024, n_mels=40)\n",
        "print(mel_basis.shape)\n",
        "mel_S = np.dot(mel_basis, D)  # (40,513)(513,327)\n",
        "mel_S.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKVegj1hcAod"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 4));\n",
        "\n",
        "plt.subplot(1, 3, 1);\n",
        "librosa.display.specshow(mel_basis, sr=sr, hop_length=512, x_axis='linear');\n",
        "plt.ylabel('Mel filter');\n",
        "plt.colorbar();\n",
        "plt.title('1. Our filter bank for converting from Hz to mels.');\n",
        "\n",
        "plt.subplot(1, 3, 2);\n",
        "mel_10 = librosa.filters.mel(sr=sr, n_fft=1024, n_mels=10)\n",
        "librosa.display.specshow(mel_10, sr=sr, hop_length=512, x_axis='linear');\n",
        "plt.ylabel('Mel filter');\n",
        "plt.colorbar();\n",
        "plt.title('2. Easier to see what is happening with only 10 mels.');\n",
        "\n",
        "plt.subplot(1, 3, 3);\n",
        "idxs_to_plot = [5, 10, 15, 20, 25, 30]\n",
        "for i in idxs_to_plot:\n",
        "    plt.plot(mel_basis[i]);\n",
        "plt.legend(labels=[f'{i+1}' for i in idxs_to_plot]);\n",
        "plt.title('3. Plotting some triangular filters separately.');\n",
        "\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5m9DBzOX34o"
      },
      "outputs": [],
      "source": [
        "def hertz_to_mel(freq):\n",
        "    return 2595.0 * np.log10(1 + (freq/700.0))\n",
        "\n",
        "def mel_to_hertz(mel):\n",
        "    return 700.0 * (10**(mel/2595.0)) - 700.0\n",
        "\n",
        "def melfrequencies_mel_filterbank(num_bands, freq_min, freq_max, num_fft_bands):\n",
        "    mel_max = hertz_to_mel(freq_max)\n",
        "    mel_min = hertz_to_mel(freq_min)\n",
        "    delta_mel = abs(mel_max - mel_min) / (num_bands + 1.0)\n",
        "    frequencies_mel = mel_min + delta_mel*np.arange(0, num_bands+2)\n",
        "    lower_edges_mel = frequencies_mel[:-2]\n",
        "    upper_edges_mel = frequencies_mel[2:]\n",
        "    center_frequencies_mel = frequencies_mel[1:-1]\n",
        "\n",
        "    return center_frequencies_mel, lower_edges_mel, upper_edges_mel\n",
        "\n",
        "def compute_melmat(num_mel_bands=12, freq_min=64, freq_max=8000,\n",
        "                   num_fft_bands=513, sample_rate=16000):\n",
        "    center_frequencies_mel, lower_edges_mel, upper_edges_mel =  \\\n",
        "        melfrequencies_mel_filterbank(\n",
        "            num_mel_bands,\n",
        "            freq_min,\n",
        "            freq_max,\n",
        "            num_fft_bands\n",
        "    )\n",
        "\n",
        "    len_fft = float(num_fft_bands) / sample_rate\n",
        "    center_frequencies_hz = mel_to_hertz(center_frequencies_mel)\n",
        "    lower_edges_hz = mel_to_hertz(lower_edges_mel)\n",
        "    upper_edges_hz = mel_to_hertz(upper_edges_mel)\n",
        "    freqs = np.linspace(0.0, sample_rate/2.0, num_fft_bands)\n",
        "    melmat = np.zeros((num_mel_bands, num_fft_bands))\n",
        "\n",
        "    for imelband, (center, lower, upper) in enumerate(zip(\n",
        "            center_frequencies_hz, lower_edges_hz, upper_edges_hz)):\n",
        "\n",
        "        left_slope = (freqs >= lower)  == (freqs <= center)\n",
        "        melmat[imelband, left_slope] = (\n",
        "            (freqs[left_slope] - lower) / (center - lower)\n",
        "        )\n",
        "\n",
        "        right_slope = (freqs >= center) == (freqs <= upper)\n",
        "        melmat[imelband, right_slope] = (\n",
        "            (upper - freqs[right_slope]) / (upper - center)\n",
        "        )\n",
        "\n",
        "    return melmat, (center_frequencies_mel, freqs)\n",
        "\n",
        "f1, f2 = 1000, 8000\n",
        "melmat, (melfreq, fftfreq) = compute_melmat(6, f1, f2, num_fft_bands=4097)\n",
        "fig, ax = plt.subplots(figsize=(8, 3))\n",
        "ax.plot(fftfreq, melmat.T)\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('Weight')\n",
        "ax.set_xlabel('Frequency / Hz')\n",
        "ax.set_xlim((f1, f2))\n",
        "ax2 = ax.twiny()\n",
        "ax2.xaxis.set_ticks_position('top')\n",
        "ax2.set_xlim((f1, f2))\n",
        "ax2.xaxis.set_ticks(mel_to_hertz(melfreq))\n",
        "ax2.xaxis.set_ticklabels(['{:.0f}'.format(mf) for mf in melfreq])\n",
        "ax2.set_xlabel('Frequency / mel')\n",
        "plt.tight_layout()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.matshow(melmat)\n",
        "plt.axis('equal')\n",
        "plt.axis('tight')\n",
        "plt.title('Mel Matrix')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj61ViNzbKdI"
      },
      "outputs": [],
      "source": [
        "import librosa.display\n",
        "\n",
        "S = librosa.feature.melspectrogram(y=audio_np, sr=sr, n_mels = 256)\n",
        "log_S = librosa.power_to_db(S, ref=np.max)\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
        "plt.title('Mel power sepctrogram')\n",
        "plt.colorbar(format='%+02.0f dB')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGldHcsI63Jp"
      },
      "outputs": [],
      "source": [
        "import librosa.display\n",
        "\n",
        "S = librosa.feature.melspectrogram(y=complexwave, sr=sr, n_mels = 256)\n",
        "log_S = librosa.power_to_db(S, ref=np.max)\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
        "plt.title('Mel power sepctrogram')\n",
        "plt.colorbar(format='%+02.0f dB')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXDcpxzn9Oe2"
      },
      "source": [
        "\n",
        "### Bark scale\n",
        "귀가 인식하는 주파수의 영역은 대략 $20Hz~2000Hz$로 가정합니다. 하지만 주파수에 대한 사람의 인식은 비선형적입니다. 귀와 뇌의 가청대역을 24개의 대역으로 나눈것을 Bark라고 합니다! Bark scale은 $500Hz$ 이하에서는 $100Hz$의 대역폭을 가지며, $500Hz$ 이상에서는 각 대역의 중심수파수의 대략 20%에 해당하는 대역폭을 가지게 됩니다.\n",
        "\n",
        "`20, 100, 200, 300, 400, 510, 630, 770, 920, 1080, 1270, 1480, 1720, 2000, 2320, 2700, 3150, 3700, 4400, 5300, 6400, 7700, 9500, 12000, 15500 ( Hz )`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioyr5bJZ9ZI4"
      },
      "source": [
        "### Log compression\n",
        "\n",
        "$$ 10 * log10(\\frac{S}{ref})$$\n",
        "의 단위로 신호를 스케일링 합니다. 이는 spectrogram을 데시벨 유닛으로 전환해 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtB9oxRf9aWg"
      },
      "outputs": [],
      "source": [
        "#log compression\n",
        "log_mel_S = librosa.power_to_db(mel_S)\n",
        "log_mel_S.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZEIi_aS9Tpf"
      },
      "source": [
        "### Discrete cosine transform (DCT)\n",
        "\n",
        "DCT는 n개의 데이터를 n개의 코사인 함수의 합으로 표현하여 데이터의 양을 줄이는 방식입니다.\n",
        "\n",
        "- 저 주파수에 에너지가 집중되고 고 주파수 영역에 에너지가 감소합니다.\n",
        "\n",
        "Filter Bank는 모두 Overlapping 되어 있기 때문에 Filter Bank 에너지들 사이에 상관관계가 존재하기 때문이다. DCT는 에너지들 사이에 이러한 상관관계를 분리 해주는 역활을 해줍니다.\n",
        "\n",
        "하지만 여기서 26개 DCT Coefficient 들 중 12만 남겨야 하는데, 그 이유는 DCT Coefficient 가 많으면, Filter Bank 에너지의 빠른 변화를 나타내게 되고, 이것은 음성인식의 성능을 낮추게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-8kRCBs9K1v"
      },
      "outputs": [],
      "source": [
        "# mfcc (DCT)\n",
        "mfcc = librosa.feature.mfcc(S=log_mel_S, n_mfcc=12)\n",
        "mfcc = mfcc.astype(np.float32)    # to save the memory (64 to 32 bits)\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(mfcc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEVvx4wV9U7Q"
      },
      "outputs": [],
      "source": [
        "mfcc = librosa.feature.mfcc(S=log_mel_S, n_mfcc=12)\n",
        "delta2_mfcc = librosa.feature.delta(mfcc, order=8)\n",
        "print(delta2_mfcc.shape)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(delta2_mfcc)\n",
        "plt.ylabel('MFCC coeffs')\n",
        "plt.xlabel('Time')\n",
        "plt.title('MFCC')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XkHRo5MIXdg"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.uniform()"
      ],
      "metadata": {
        "id": "Foj4_Q9CeEg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jJQc7SZ67K_"
      },
      "outputs": [],
      "source": [
        "def change_pitch(data, sr):\n",
        "    y_pitch = data.copy()\n",
        "    bins_per_octave = 12\n",
        "    pitch_pm = 4\n",
        "    pitch_change = pitch_pm * 2 * (np.random.uniform())\n",
        "    # pitch_change = pitch_pm\n",
        "    y_pitch = librosa.effects.pitch_shift(y_pitch.astype('float64'), sr=sr, n_steps=pitch_change,\n",
        "                                          bins_per_octave=bins_per_octave)\n",
        "    return y_pitch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEXy6iCw-cQz"
      },
      "outputs": [],
      "source": [
        "def waveform_aug(waveform, sr):\n",
        "  y = change_pitch(waveform, sr)\n",
        "  fig = plt.figure(figsize = (14,5))\n",
        "  librosa.display.waveshow(y, sr=sr)\n",
        "  ipd.display(ipd.Audio(data=y, rate=sr))\n",
        "  return y, sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Upy4sn9P_ad-"
      },
      "outputs": [],
      "source": [
        "ipd.display(ipd.Audio(data=audio_np, rate=sr))\n",
        "y, sr = waveform_aug(audio_np, 16000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.display(ipd.Audio(data=sin_c, rate=sr))\n",
        "y, sr = waveform_aug(sin_c, 16000)"
      ],
      "metadata": {
        "id": "3tKVZTW6d5qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WE-Lo0eAAbt"
      },
      "outputs": [],
      "source": [
        "S = librosa.feature.melspectrogram(y=audio_np, sr=sr, n_mels = 128)\n",
        "log_S = librosa.power_to_db(S, ref=np.max)\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
        "plt.title('Mel power sepctrogram')\n",
        "plt.colorbar(format='%+02.0f dB')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-c8REBLEGvH"
      },
      "outputs": [],
      "source": [
        "np.random.uniform(low=1.5, high=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTyUpzm_6-jG"
      },
      "outputs": [],
      "source": [
        "def value_aug(data):\n",
        "    y_aug = data.copy()\n",
        "    dyn_change = np.random.uniform(low=1.5, high=3)\n",
        "    y_aug = y_aug * dyn_change\n",
        "    return y_aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM5tYX9D7LCq"
      },
      "outputs": [],
      "source": [
        "def add_noise(data):\n",
        "    noise = np.random.randn(len(data))\n",
        "    data_noise = data + 0.005 * noise\n",
        "    return data_noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHDlVavP7OeW"
      },
      "outputs": [],
      "source": [
        "def hpss(data):\n",
        "    y_harmonic, y_percussive = librosa.effects.hpss(data.astype('float64'))\n",
        "    return y_harmonic, y_percussive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IRdPB_ZLEUR"
      },
      "outputs": [],
      "source": [
        "a = np.arange(10)\n",
        "np.roll(a,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-mo_mbs7dXm"
      },
      "outputs": [],
      "source": [
        "def shift(data):\n",
        "    return np.roll(data, 1600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zm41vQo7emj"
      },
      "outputs": [],
      "source": [
        "def stretch(data, rate=1):\n",
        "    input_length = len(data)\n",
        "    streching = librosa.effects.time_stretch(y=data, rate=rate)\n",
        "    if len(streching) > input_length:\n",
        "        streching = streching[:input_length]\n",
        "    else:\n",
        "        streching = np.pad(streching, (0, max(0, input_length - len(streching))), \"constant\")\n",
        "    return streching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwZ4sndO70rj"
      },
      "outputs": [],
      "source": [
        "def change_pitch_and_speed(data):\n",
        "    y_pitch_speed = data.copy()\n",
        "    # you can change low and high here\n",
        "    length_change = np.random.uniform(low=0.8, high=1)\n",
        "    speed_fac = 1.0 / length_change\n",
        "    tmp = np.interp(np.arange(0, len(y_pitch_speed), speed_fac), np.arange(0, len(y_pitch_speed)), y_pitch_speed)\n",
        "    minlen = min(y_pitch_speed.shape[0], tmp.shape[0])\n",
        "    y_pitch_speed *= 0\n",
        "    y_pitch_speed[0:minlen] = tmp[0:minlen]\n",
        "    return y_pitch_speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8iaUKZk8BHq"
      },
      "outputs": [],
      "source": [
        "data_noise = add_noise(audio_np)\n",
        "data_roll = shift(audio_np)\n",
        "data_stretch = stretch(audio_np)\n",
        "pitch_speed = change_pitch_and_speed(audio_np)\n",
        "value = value_aug(audio_np)\n",
        "y_harmonic, y_percussive = hpss(audio_np)\n",
        "y_shift = shift(audio_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DFAEQw1Rxc7"
      },
      "outputs": [],
      "source": [
        "ipd.Audio(y_percussive, rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziH9olL3_lx9"
      },
      "outputs": [],
      "source": [
        "train_audio_transforms = torch.nn.Sequential(\n",
        "      torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "      torchaudio.transforms.FrequencyMasking(freq_mask_param=15, iid_masks=False),\n",
        "      torchaudio.transforms.TimeMasking(time_mask_param=35, iid_masks=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vw5QKX9AnpA"
      },
      "outputs": [],
      "source": [
        "masking_freq = train_audio_transforms(test_dataset[0][0])\n",
        "masking_freq.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWuu534KI8Be"
      },
      "outputs": [],
      "source": [
        "S = librosa.feature.melspectrogram(y=audio_np, sr=sr, n_mels = 128)\n",
        "log_S = librosa.power_to_db(masking_freq[0,:,:].numpy(), ref=np.max)\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
        "plt.title('Mel power sepctrogram')\n",
        "plt.colorbar(format='%+02.0f dB')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZYUk2bODiz"
      },
      "source": [
        "## Make filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRzGFjtVTIL9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def butter_pass(cutoff, fs, btype, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype=btype, analog=False)\n",
        "    return b, a\n",
        "\n",
        "def butter_filter(data, cutoff, fs, btype, order=5):\n",
        "    b, a = butter_pass(cutoff, fs, btype, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0QPT9M5TJzS"
      },
      "outputs": [],
      "source": [
        "# Filter requirements.\n",
        "order = 6\n",
        "fs = 16000     # sample rate, Hz\n",
        "cutoff = 600  # desired cutoff frequency of the filter, Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw2h1Hv_TMmm"
      },
      "outputs": [],
      "source": [
        "# Get the filter coefficients so we can check its frequency response.\n",
        "b, a = butter_pass(cutoff, fs, 'lowpass',order)\n",
        "\n",
        "# Plot the frequency response.\n",
        "w, h = freqz(b, a, worN=8000)\n",
        "\n",
        "plt.plot(0.5*fs*w/np.pi, np.abs(h), 'b')\n",
        "plt.plot(cutoff, 0.5*np.sqrt(2), 'ko')\n",
        "plt.axvline(cutoff, color='k')\n",
        "plt.xlim(0, 0.5*fs)\n",
        "plt.title(\"Pass Filter Frequency Response\")\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk2dqmTfTN5c"
      },
      "outputs": [],
      "source": [
        "T = len(audio_np)/sr   # seconds\n",
        "n = int(T * sr) # total number of samples\n",
        "t = np.linspace(0, T, n, endpoint=False)\n",
        "\n",
        "# Filter the data, and plot both the original and filtered signals.\n",
        "y = butter_filter(audio_np, cutoff, fs, 'lowpass', order)\n",
        "ipd.Audio(y, rate=fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU6Udf0ITX7_"
      },
      "outputs": [],
      "source": [
        "X = librosa.core.stft(y, n_fft=1024, hop_length=512, win_length=1024) # get frequency to STFT\n",
        "S = librosa.amplitude_to_db(abs(X)) # Change Amplitude to decibel\n",
        "plt.figure(figsize=(15, 5)) # get figure size\n",
        "librosa.display.specshow(S, sr=fs, hop_length=512, x_axis='time', y_axis='log') #display\n",
        "plt.colorbar(format='%+2.0f dB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc8Gd6sVPDr8"
      },
      "outputs": [],
      "source": [
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0][0].shape"
      ],
      "metadata": {
        "id": "759mp1ZritXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0aaPO_8TcWJ"
      },
      "outputs": [],
      "source": [
        "train_audio_transforms = torch.nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtIkXO0R_66c"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = []\n",
        "a = np.array([1,2,3,4])\n",
        "l1.append(a)\n",
        "b = np.array([5,6,7,8])\n",
        "l1.append(b)\n",
        "print(l1)\n",
        "print(type(np.array(l1)))"
      ],
      "metadata": {
        "id": "zWjPWRMxjsEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMITaH3ALmr8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "def load_dataset(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, _, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "\n",
        "        # print(spec.shape)\n",
        "        spectrograms.append(spec)\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "def get_dataloader(data):\n",
        "    x_train = load_dataset(data, \"train\")\n",
        "    x_valid = load_dataset(data, 'valid')\n",
        "\n",
        "    # print(type(x_train))\n",
        "    # print(np.asarray(x_train).shape)\n",
        "    # return\n",
        "    mean = np.mean(x_train)\n",
        "    std = np.std(x_train)\n",
        "    x_train = (x_train - mean)/std\n",
        "    x_valid = (x_valid - mean)/std\n",
        "\n",
        "    train_set = Dataset(x_train)\n",
        "    vaild_set = Dataset(x_valid)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=4, shuffle=True, drop_last=False)\n",
        "    valid_loader = DataLoader(vaild_set, batch_size=4, shuffle=False, drop_last=False)\n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0SbXgFKK4tkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}